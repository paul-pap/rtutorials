---
title: "Regression"
output:
  learnr::tutorial:
    language: de
    css: css/boxes.css
    fig_caption: no
runtime: shiny_prerendered
bibliography: ref.json
link-citations: TRUE
description: Einführung in die einfache lineare Regression.
resource_files:
- css/boxes.css
---

```{r setup, include=FALSE}
library(learnr)
library(ggplot2)
library(shiny)
knitr::opts_chunk$set(echo = FALSE)

if(require(ggbrace)) library(ggbrace) else {devtools::install_github("NicolasH2/ggbrace")}
library(ggbrace)
```

## Inhalt

-   Vorannahmen der Regression prüfen

## Lernziele

## Teaser lineare Regression

### Ein Beispiel

Als Förster\*in in den USA stehst du vor einer spätblühenden Traubenkirsche (*Prunus serotina*) ([wiki](https://de.wikipedia.org/wiki/Sp%C3%A4tbl%C3%BChende_Traubenkirsche)). Sie soll bald gefällt werden, denn sie hat ein begehrtes Holz, und du brauchst Geld. Am liebsten möchtest wissen, bevor du den Baum fällst, wie viele Kubikmeter Holz wohl dabei herauskommen werden, die du verkaufen kannst. Aber du möchtest keine allzu komplexen Messungen vornehmen. Den besten und einfachsten Schätzer, den wir bequem vornehmen können ist der Durchmesser des Baumes, der sich ungefähr aus dem Umfang errechnen lässt. Dafür braucht es lediglich ein Maßband und $\pi$.

Der Baum, vor dem wir stehen, hat einen Durchmesser von 45 cm. Wie viele Kubikmeter Holz können wir erwarten?

![](images/tree_patrick.jpg){width=60%}

Bild: Kein Kirschbaum, aber zeigt das Prinzip.

*US Army Corps of Engineers. Patrick Bloodgood, photographer. [CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia Commons*

::: infobox
Achtung! **Vereinfachungsalarm**

Wir lassen hier einige Dinge der Einfachheit halber komplett außer Acht, zum Beispiel:

-   in der Forstwirtschaft wird zur Schätzung des Volumens eine etwas kompliziertere Formel verwendet, in die neben dem Durchmesser auch die Höhe mit einfließt
-   Dabei gibt es verschiedene Volumenmaße für Holz mit und ohne Rinde
-   ...
:::

### Beispieldaten

Wir haben bereits Daten von 31 gefällten Bäumen der gleichen Art, und können damit eine Vorhersage treffen. Die Daten befinden sich im `trees`-Datensatz, der in Base R eingebaut ist, und sind der Grund für dieses Beispiel.

Schauen wir uns die Daten mal an:

::: aufgabe
**1.** Wie sehen die ersten 6 Zeilen des Datensatzes `trees` aus?
:::

```{r head, exercise = TRUE}

```

```{r head-solution}
head(trees)
```

#### Ein bisschen Data-Cleaning

::: aufgabe
**2.**

Die Höhe `Height` lassen wir heute außen vor.

Der Durchmesser heißt im Datensatz fälschlicherweise `Girth`, was Umfang bedeutet. (Das ist einfach ein Fehler in der Benennung, siehe `?trees`).

Beheben Sie das, in dem Sie in `trees` eine neue Variable namens `Diameter` erstellen, die den Inhalt der Variable `Girth` enthält.
:::

```{r girth, exercise = TRUE}

```

```{r girth-solution}
trees$Diameter <- trees$Girth
```

::: aufgabe
**3.**

Die Variablen sind (typisch USA) alle in nicht-metrischen Einheiten angegeben.

Rechnen Sie `Diameter` und `Girth` in metrische Einheiten um, damit das Beispiel intuitiver verständlich ist.

`Diameter` = Inch. Ziel: cm

`Volume` = ft³. Ziel: m³

**Hilfestellung zur Umrechung:**

$cm = Inch * 2.54$

$m^3 = ft^3 * 0.0283168466$
:::

```{r feet-setup}
trees$Diameter <- trees$Girth
```

```{r feet, exercise = TRUE}

```

```{r feet-solution}
trees$Diameter <- trees$Diameter * 2.54 # Inch in cm
trees$Volume <- trees$Volume * 0.0283168466 #  ft³ in m³

```

```{r silentsetup}
# Hier drauf sollten sich alle Exercise Code Chunks, die das trees-Dataset verwenden, beziehen

trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³

trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)

fit <- lm(Volume ~ Diameter, data = trees)

trees_predict <- trees["Volume"]
trees_predict$predict <- predict(fit)
```

```{r silentsetup-global}
# Und das gleiche noch mal fürs Global Environment

trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³

trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)
```

#### Visualisierung

Ein schneller Scatterplot gibt uns jetzt Auskunft über die Beziehung zwischen Durchmesser und Volumen:

```{r, echo = TRUE}
ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")
```

Grundsätzlich sieht es so aus, dass wir mehr Holz ernten, je dicker der Baum war.

Durchmesser und Volumen hängen also irgendwie proportional zusammen. Für unsere Vorhersage wünschen wir uns aber zu wissen, wie genau diese Proportion aussieht.

Glücklicherweise sieht der Zusammenhang linear aus, das heißt, wir könnten ihn sinnvoll mit einer Gerade beschreiben:

```{r echo=F, message=FALSE, warning=FALSE}
ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")
```

Die blaue Gerade ist unser Modell, die Punkte sind die Realität, aus der wir das Modell abgeleitet haben. Ein Modell wird den Komplexitäten der Realität nie komplett gerecht, aber manche Modelle sind trotzdem sehr nützlich. Die Gerade hier zum Beispiel ist ein eher einfaches Modell und beschreibt die Realität trotzdem relativ gut.

Geraden werden beschrieben durch lineare Funktionen. Unser Modell besteht also essentiell aus einer linearen Funktion! Wie genau das alles funktioniert, erfahrt ihr im nächsten Abschnitt. Vorher aber noch ein kleiner Test, ob ihr auch schon mit dem Modell umgehen könnt und die Eingangsfrage lösen könntet:

```{r first}
question_numeric(
  "Wie viel Kubikmeter Holz sind zu erwarten laut der blauen Gerade (unserem linearen Modell) bei einem Durchmesser von 45 cm?",
  answer(1.5, correct = T)
)
```

Super, das waren die ersten Schritte! Im nächsten Kapitel werden die mathematischen Grundlagen aufgefrischt.

![Yay!](./images/shuffling_tree.gif)

## lineare Funktionen

![](./images/function.gif)

Bisher sieht unser Modell so aus:

Wir wissen nicht genau, was drin passiert, nur das wir den **Prädiktor** $x$ (Durchmesser) reingeben und daraus das **Kriterium** $y$ (Volumen) herausbekommen wollen. In diesem Kapitel erklären wir, was im Modell vorgeht - nämlich eine lineare Funktion.

### ein bisschen Begriffsarbeit

Die lineare Funktion kennen sicher alle noch aus der Schule, ca. 8. Klasse:

$$
y = mx + n
$$

oder in anderer Notation:

$$
y = a + bx
$$

Egal welche der Notationen Sie hatten, oder auch wenn Sie noch gar keine Berührungspunkte mit linearen Funktionen hatten, der Aufbau ist immer gleich.

Wir werden diese Notation verwenden:

$$
\hat y_i = b_0 + b_1\cdot x_i
$$

-   Dabei steht $\hat y_i$ für den durch das Modell vorhergesagten Wert, auch engl. "*response variable"* genannt,"*output variable"* oder **Kriterium**. Im Beispiel wäre es das Holzvolumen in m³.

    Ein Dach über der Variable kennzeichnet immer, dass es sich hier um einen geschätzten Wert handelt.

-   $x_i$ ist der **Prädiktor**, oder unsere *input variable*, im Beispiel der Baumdurchmesser in Inches.

-   Der tiefgestellte Index $i$ bezeichnet die Nummer eines Messwertpaars

Daraus ergibt sich für uns folgendes Modell:

$$
\hat{Volumen_i} = b_0 + b_1 \cdot Durchmesser_i 
$$

#### Was sind $b_0$ und $b_1$?

Beides sind Regressionskoeffizienten - zwei Parameter, die die Lage der Gerade beschreiben.

-   $b_0$ ist die **Regressionskonstante**.

    Das ist der vorhergesagte Wert, wenn $x$ 0 ist. Das ergibt sich aus der Gleichung, wenn man für $x$ 0 einsetzt:

    $\hat y_i = b_0 + b_1 \cdot 0 = b_0$

    Bei einer Gerade ist $b_0$ also dort, wo $x = 0$. Das ist oft auch dort, wo die y-Achse verläuft, deswegen nennt man $b_0$ auch y-Achsenabschnitt oder *Intercept* auf Englisch.

-   $b_1$ ist das **Regressionsgewicht**.

    Es entspricht der Steigung der Gerade (engl. *Slope*) und ist inhaltlich die Änderung im vorhergesagten Wert ($\hat y_i$), wenn man $x$ um eine Einheit erhöht.

    Es erinnern sich bestimmt alle an das Dreieck, was man an die Gerade zeichnen kann, um die Steigung zu bestimmen.

### Beispiel-Gerade

Hier einmal eine Beispiel-Gerade, beschrieben durch die Gleichung $\hat y_i = 9 - 1.5x_i$.

-   Intercept $b_0 = 9$
-   Steigung $b_1 = -1.5$

```{r}
.gerade <- function(x, intercept = 0, slope = 1){
  intercept + slope * x
}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(-1, 10), args = list(slope = -1.5, intercept = 9)) +
  geom_segment(aes(x = 2, y = 6, yend = 6, xend = 3), color = "red", 
               arrow = arrow(length = unit(0.03, "npc"))) +
  geom_segment(aes(x = 3, xend = 3, y = 6, yend = 4.5), color = "red",
               arrow = arrow(length = unit(0.03, "npc"))) +
  annotate("text", x = 2.5, y = 6.5, label = "1") +
  annotate("text", x = 3.5, y = 5.125, label = "-1.5") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 10), expand = F) +
  scale_y_continuous(breaks = 0:10) +
  scale_x_continuous(breaks = 0:10) +
  theme_minimal()
```

### Quiz

```{r}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 5, slope = 0.5)) +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 10, slope = 1), linetype = "dashed") +
  coord_cartesian(ylim = c(0, 20), xlim = c(0, 20)) +
  theme_minimal()
```

```{r guess}
quiz(
  question_numeric(
  "Welche Steigung (b1) hat die gestrichelte Linie?",
  answer(1, correct = T)
  ),
  question_numeric(
  "Welche Steigung (b1) hat die durchgezogene Linie?",
  answer(0.5, correct = T)
  ),
  question_numeric(
    "Welches Intercept (b0) hat die gestrichelte Linie?",
    answer(10, correct = T)
  ),
  question_numeric(
    "Welches Intercept (b0) hat die durchgezogene Linie?",
    answer(5, correct = T)
  ),
  caption = "Schulmathe auffrischen"
)
```

Alles klar! Wir sind bereit, $b_0$ und $b_1$ für unser eigenes Modell zu bestimmen.

## Fitting the model

Nun wissen wir also, dass das Modell einer einfachen linearen Regression einfach eine Geradengleichung ist, beschrieben durch die Parameter $b_0$ (Intercept) und $b_1$ (Steigung.)

Aber wie genau kommen wir auf diese Parameter? Den Prozess, ein Modell zu erstellen, was die Realität möglichst gut erklärt, nennt man auf Englisch *to fit a model*.

$b_0$ und $b_1$ werden so gewählt, dass die Regressionsgerade einen möglichst kleinen Abstand zu allen Datenpunkten hat.

::: aufgabe
Probieren Sie selbst aus, die Regressionsgerade optimal durch die Datenpunkte zu legen!
:::

</br>


```{r shiny_ui, echo=FALSE}
sliderInput("b0", "Intercept b0:", min = -1.5, max = 1.5, value = 1, step = 0.0001)
sliderInput("b1", "Slope b1:", min = 0, max = 0.15, value = 0, step = 0.0001)
plotOutput("distPlot")
actionButton("show", "Antwort einreichen & Lösung anzeigen", class = "btn-default")
tableOutput("estimate")
plotOutput("optimumPlot")
```

```{r shiny_server, context="server"}
  # Prerequisites: Daten müssen noch mal erstellt werden weil shiny app verwendet eigenes environment
  require(ggplot2)
  # sysinfo <- Sys.info()
  # if(sysinfo["sysname"] == "Linux") require(Cairo) #bessere Grafik unter Linux, ist aber nur hörensagen und nicht nötig

  trees$Diameter <- datasets::trees$Girth * 2.54 # Inch in cm
  trees$Volume <- datasets::trees$Volume * 0.0283168466 #  ft³ in m³
  fit <- lm(Volume ~ Diameter, data = trees)

  # Wahre Werte für b0 und b1 bestimmen
  coefs <-  fit |> coef() |> as.numeric()

  # Minimale QS_res bestimmen
  sum_res_min <- (trees$Volume - predict(fit))^2 |> sum()

  # Lösungsplot
  optimumPlot <- ggplot(trees, aes(x = Diameter, y = Volume)) +
    geom_point() +
    geom_abline(intercept = coefs[1], slope = coefs[2], color = "blue") +
    geom_linerange(x = trees$Diameter, ymin = trees$Volume, ymax = predict(fit),
                   linetype = "solid", colour = "red") +
    annotate(geom = "label", x = 50, y = 0.3, hjust = 0, col = "red", size = 5,
             label = paste0("Summe der \nquadrierten\nResiduen \n=", round(sum_res_min, 4))) +
    theme_minimal() +
    labs(x = "Durchmesser (cm)", y = "Volumen (m³)", title = "beste Schätzung") +
    coord_cartesian(xlim = c(20, 55), ylim =c(0, 2.5), expand = F)

  # Reaktive Vorhersage (für rote Linien)
  pre <- reactive(input$b0 + input$b1 * trees$Diameter)

  # reaktive Summe der quadrierten Residuen
  res <- reactive(sum((trees$Volume - pre()) ^ 2))

  # Grading Function
  .estimate_grader <- function(x){
    real <- c(coefs, sum_res_min)
    deviation <- (x - real) * 100 / real
    data.frame("Ihre Schätzung" = x,
               "beste Schätzung" = real,
               "Abweichung" = paste(round(deviation, 2), "%"),
               row.names = c("Intercept", "Slope", "summierte quadrierte Residuen"),
               check.names = FALSE)
  }

  # Check Button Logic
  observeEvent(input$show, {
    estimate_cache <- c(input$b0, input$b1, res())
    # updateSliderInput(inputId = "b0", value = coefs[1])
    # updateSliderInput(inputId = "b1", value = coefs[2])
    output$estimate <- renderTable(.estimate_grader(estimate_cache),
                                   rownames = TRUE, digits = 4)
    output$optimumPlot <- renderPlot(optimumPlot)
  })

  # Plot
  output$distPlot <- renderPlot({

  ggplot(trees, aes(x = Diameter, y = Volume)) +
    geom_point() +
    geom_abline(intercept = input$b0, slope = input$b1, color = "blue") +
    geom_linerange(x = trees$Diameter, ymin = trees$Volume, ymax = pre(),
                   linetype = "solid", colour = "red") +
    annotate(geom = "label", x = 50, y = 0.3, hjust = 0, col = "red", size = 5,
             label = paste0("Summe der \nquadrierten\nResiduen \n=", round(res(), 5))) +
    theme_minimal() +
    labs(x = "Durchmesser (cm)", y = "Volumen (m³)") +
    coord_cartesian(xlim = c(20, 55), ylim =c(0, 2.5), expand = F)

})
```

</br>

Hervorragend! Sie haben jetzt hoffentlich ein intuitives Gefühl für die Problemstellung bekommen.

Wie Sie gesehen haben, gab es auch eine durch den Computer berechnete „beste Schätzung“ - vielleicht haben Sie das einfach so hingenommen, aber was qualifiziert diese Schätzung eigentlich dazu, sich die „beste“ zu nennen?

Ein wichtiges Konzept sind dabei die Residuen. 

### Residuen

Die oben in rot gekennzeichneten vertikalen Abstände zwischen realen Werten und den vorhergesagten Werten nennt man auch **Residuen**, also "Überbleibsel". Residuen stellen die Abweichung der Realität vom Modell dar, also den "Fehler", den das Modell nicht erklären kann.

Offensichtlich ist es daher besser, wenn die Residuen kleiner sind, als wenn sie sehr groß sind. Kleine Residuen bedeuten, dass das Modell näher an der Realität liegt.

#### Formel

Ein Residuum $e$ (wie Error) der Messung $i$ errechnet sich aus: 
$$e_i = y_i - \hat y_i$$.

mit: 

-   $y_i$ = tatsächlicher Wert

-   $\hat y_i$ = vorhergesagter Wert

#### Methode der kleinsten Quadrate

Eine Möglichkeit, zu definieren wann eine Regressionsgerade optimal an die Daten angepasst ist, zu sagen:

> „Die Regressionsgerade liegt optimal, wenn die Summe der quadrierten Residuen minimal ist.“

$$
\sum_{i = 1}^n \left(e_i\right)^2 = min
$$ 


Dahinter steht ein grundlegendes Konzept, die „Methode der kleinsten Quadrate“ (*least squares*), was eine weit verbreitete Schätzmethode ist. 

Natürlich hat *least squares estimation* auch Nachteile, zum Beispiel durch das Quadrieren hohe Empfindlichkeit gegenüber Extremwerten. Deswegen sei hier gesagt: es gibt auch andere Methoden, jedoch ist *least squares* der Standard.

<details>
<summary><a>▼ * Hintergründe: analytische Lösung der Minimierung</a></summary>
::: infobox

##### Woher kommen die Formeln für $b_1$ und $b_0$?

Das tolle an der Methode der kleinsten Quadrate ist, dass es für die Regressionskoeffizienten eine analytische Lösung gibt, das heißt, mathematische Formeln, mit denen sie *garantiert* so bestimmt werden, dass die quadrierten Residuen minimal sind. 

Das hier ist die Ausgangslage, das Kriterium für eine optimal liegende Gerade: Die Summe der quadrierten Residuen soll minimal sein. 

$$
\sum_{i = 1}^n \left(e_i\right)^2 = \sum_{i = 1}^n \left( y_i - \hat y_i \right)^2 = min
$$ 

Da quadrierte Werte immer positiv sind, erreicht man, dass sich negative und positive Abweichungen nicht aufheben beim Summieren. Außerdem stellt das Quadrieren weitere günstige mathematische Eigenschaften für das Minimieren her.

##### Einsetzen der Regressionsgleichung für $\hat y$:

$$
\sum_{i = 1}^n \left( y_i - (b_0 + b_1 \cdot x_i) \right)^2 = min
$$

##### Minimieren: 

Sie haben vermutlich an den Schiebereglern selbst bemerkt - es funktioniert nicht, nur einen der beiden Koeffizienten isoliert zu optimieren, sondern sie müssen verschiedene Kombinationen ausprobieren. 

Ähnlich ist es bei der Herleitung allgemeiner Formeln. Sie funktioniert über das Nullsetzen der beiden partiellen Ableitungen nach $b_0$ und nach $b_1$. Es gibt gute externe Quellen dazu, z. B. im [wikibook über lineare Regression](https://de.wikibooks.org/wiki/Statistik:_Regressionsanalyse#Einfaches_lineares_Regressionsmodell).

Daraus leiten sich letzten Endes folgende Formeln ab:

```{=tex}
\begin{align}
b_1 &= \frac{cov(x,y)}{var(x)}
\\
b_0 &= \bar y - b_1 \cdot \bar x
\end{align}
```

Der Key-Takeaway ist: Diese Formeln liefern per mathematischer Definition die **optimale Regressionsgerade mit minimalen Residuen**!

Amazing.

:::
</details>
</br>
<details>
<summary><a>▼ \* Exkurs: iterative Lösung der Minimierung</a></summary>
::: infobox

Das Gegenstück zu einer analytischen Lösung ist ein iterativer Ansatz, also „Trial & Error“, wie Sie es oben selbst vermutlich auch gemacht haben. Das ist ebenso eine weit verbreitete Methode, die quadrierten Residuen zu minimieren, besonders dann, wenn die mathematischen Voraussetzungen für die klassische Lösung nicht gegeben sind (das sind einige, siehe Kapitel „Voraussetzungen“). Prädestiniert für die Trial & Error-Lösung sind Machine-Learning-Algorithmen. Wir werden keine dieser Algorithmen in R verwenden, aber ich möchte darauf hinweisen, dass es das gibt.

Hier eine Illustration des Fitting-Prozesses, allerdings mit anderen Daten. Die Grafik zeigt wie ein Machine-Learning-Algorithmus sich Schritt für Schritt der optimalen Regressionsgerade annähert, und die mittleren quadrierten Residuen (Mean Squared Error, MSE) immer kleiner werden, bis es nicht kleiner geht.

![](images/iterative_fitting.gif){width="80%"}

Bild: [ghbat.com](https://gbhat.com/machine_learning/linear_regression.html)

:::
</details>
</br>

#### Übung Residuen
::: aufgabe
**1.**
Um die Formeln noch mal selbst anzuwenden, ist hier eine kleine Praxisaufgabe. 

In der Tabelle `trees_predict` finden Sie zwei Spalten: Das tatsächliche Volumen ($y_i$) und das vorhergesagte Volumen ($\hat y_i$).

a) Schauen Sie sich `trees_predict` an. 
b) Berechnen Sie eine neue Spalte mit den Residuen für jeden einzelnen Messwert!
c) Berechnen Sie die Summe der quadrierten Residuen!
:::
</br>
```{r predict, exercise = TRUE, exercise.setup = "silentsetup"}
trees_predict
```

```{r predict-solution}
# a)
head(trees_predict)

# b)
trees_predict$resid <- trees_predict$Volume - trees_predict$predict

# c)
sum(trees_predict$resid ^ 2)
```

```{r predict-question}
question_numeric("Welchen Wert haben Sie bei c) für die summierten quadrierten Residuen herausbekommen? (3 Nachkommastellen)",
                 answer(0.420, correct = T, message = "Das ist genau der gleiche Wert wie oben bei der Aufgabe mit den Schiebereglern."), tolerance = 0.001, allow_retry = T)
```

</br>

Hang on! Im nächsten Kapitel wird es praktisch: Wie kann ich ein Modell in R erstellen und mir die Koeffizienten ausrechnen lassen? Und die wichtige Frage wird vor allem sein: Wo finde ich die relevanten Ergebnisse, und wie interpretiere ich sie?

![](images/shuffling_tree.gif)

## Umsetzung in R

In R erhält man die Regressionskoeffizienten $b_0$ und $b_1$ über die Funktion `lm()` (*linear model*).

Fokussieren wir uns zunächst auf die Eingabe, den Output schauen wir uns im nächsten Schritt an.

#### Eingabe

``` r
# Modell erstellen und abspeichern 
fit <- lm(Volume ~ Diameter, data = trees)

# Modell ansehen
summary(fit)
```

##### Code Breakdown

`lm()`:

-   `Volume ~ Diameter`: ist eine Formel, die R sagt: *explain Volume by Diameter*.

    Ich finde es hilfreich, die Tilde im Kopf zu lesen als: *explained by* oder auf deutsch "erklärt durch". Links der Tilde kommt immer die Variable hin, die wir erklären oder vorhersagen wollen, und rechts der Tilde die Prädiktoren.

-   `data = trees`: Da wir die die Variablennamen verwenden ohne `trees$...` davor, sagen wir R noch im Argument `data`, wo die Variablen zu finden sind.

`fit <-`:

-   Schließlich speichern wir das Modell in einem Objekt, was wir `fit` genannt haben, was ein frei ausgewählter Name ist.

`summary(fit)`:

-   ruft eine Zusammenfassung unseres gespeicherten Modells auf, was uns zur Ausgabe führt.

#### Ausgabe

Der Output sieht eventuell überwältigend aus, weil er eine ziemlich hohe Informationsdichte hat. Das macht aber gar nichts, denn wir richten unsere Aufmerksamkeit gezielt auf den Abschnitt "Coefficients:". Dort finden wir die Parameter $b_0$ (Intercept) und $b_1$ (Steigung). Und zwar in der Spalte "Estimate".

```         
Call:
lm(formula = Volume ~ Diameter, data = trees)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.228386 -0.087972  0.004303  0.098961  0.271468 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
Diameter     0.056476   0.002758   20.48  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.1204 on 29 degrees of freedom
Multiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```

Daraus können wir entnehmen:

-   Intercept $b_0 = -1.0461$
-   Steigung $b_1 = 0.0565$

Alle anderen Spalten interessieren uns im Moment nicht.

```         
Coefficients:
             Estimate    
(Intercept) -1.046122   <--- b0
Diameter     0.056476   <--- b1
---
```

</br>
<details>
<summary><a>▼ \* Exkurs: Welchen Algorithmus verwendet `lm()`?</a></summary>
::: infobox
Lustigerweise wird unter der Haube von `lm()` letztlich nach vielen Zwischenschritten eine uralte Funktion in einer anderen Programmiersprache, nämlich FORTRAN, aufgerufen, um die Schwerarbeit zu machen. Sie stammt aus dem `LINPACK`-Paket, was original für Supercomputer in den 70ern und 80ern verfasst wurde, um lineare Gleichungssysteme zu lösen. Es ist scheinbar trotz des Alters nach wie vor effizient darin, *least squares optimization* zu betreiben. 

Hier gibt es einen interessanten Blogbeitrag [(A deep dive into how R fits a linear model)](https://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html) dazu, der jeden einzelnen Schritt auflistet bis zur untersten Ebene, dem FORTRAN-Herzstück von `lm()`. 

Der verwendete Algorithmus basiert auf QR-Zerlegung, und ist durch die Methode der „Householder-Transformationen“ implementiert. Ich weiß nicht, was das ist - aber [Wikipedia](https://de.wikipedia.org/wiki/QR-Zerlegung) weiß es :)   
:::
</details>
</br>

Jetzt sind Sie dran!

::: aufgabe
**1.**

Erstellen Sie mit `lm()` ein lineares Modell zur Erklärung des Volumens durch den Durchmesser im `trees`-Datensatz und speichern Sie es ab als `fit`.
:::

```{r model, exercise = TRUE}

```

```{r model-solution}
fit <- lm(Volume ~ Diameter, data = trees)
```

::: aufgabe
**2.**

Lassen Sie sich mit `summary()` eine Zusammenfassung des gespeicherten Modells ausgeben!

Finden Sie $b_0$ und $b_1$!
:::

```{r summary, exercise = TRUE, exercise.setup = "silentsetup"}

```

```{r summary-solution}
summary(fit)
```
</br>

![](./images/menschbaum.JPG){width=80%}

(Bild: Wikimedia Commons, CC 0)

## Interpretation der Koeffizienten

Jetzt haben wir $b_0 = -1.0461$ und $b_1 = 0.0565$ herausgefunden - aber was heißen sie eigentlich inhaltlich?

Zunächst einmal können wir uns merken, dass die Regressionskoeffizienten *immer in Einheiten der vorhergesagten Variable*, in unserem Fall also Volumen (m³) sind.

### $b_0$ (Intercept)

> „$b_0$ ist der vorhergesagte Wert, wenn der Prädiktor den Wert 0 annimt."

Bezogen auf das Beispiel:

„Wenn der Durchmesser eines Baumes 0 cm betragen würde, wäre das vorhergesagte Holzvolumen -1.05 Kubikmeter." --- Das macht inhaltlich wenig Sinn - es gibt keinen Baum, wenn er einen Durchmesser von 0 cm hat, und es gibt auch kein negatives Volumen.

Das ist mit allen Daten so, wo 0 nicht im sinnvollen Wertebereich ist. Man kann dann für eine bessere Interpretierbarkeit die Daten zentrieren, so dass 0 ein sinnvoller Wert ist. Siehe Abschnitt Transformation.

### $b_1$ (Steigung)

> „$b_1$ ist die vorhergesagte Änderung, wenn der Prädiktor um eine Einheit erhöht wird."

„Pro Zentimeter Durchmesser mehr steigt also unser erwartetes Volumen um 0.0565 Kubikmeter."

::: gelb
Das Regressionsgewicht $b_1$ ist eng verwandt mit der Korrelation, aber es gibt einen wichtigen Unterschied: Es ist nicht „symmetrisch“ wie eine Korrelation. Es lässt immer nur Rückschlüsse in eine Richtung zu, z.B. vom Durchmesser auf das Volumen, nicht umgekehrt. Das ist das Spezielle an der linearen Regression!

<details>
<summary><a>▼ Für diejenigen, die das anhand von Formeln verstehen möchten:</a></summary>

Formel für die Korrelation

$$
r = \frac{cov(x, y)}{s_x \cdot s_y}
$$
Formel für das Regressionsgewicht:

$$
b_1 = \frac{cov(x, y)}{s_x^2}
$$
Hier fehlt im Nenner die Varianz von y
</details>
</br>

:::
</br>

![](images/massband.jpg){width=80%}

## Transformationen

Für die lineare Regression müssen das Kriterium $y$ und der Prädiktor $x$ beide metrisch sein, also mindestens intervallskaliert. Ab dem Intervallskalenniveau sind Transformationen zulässig, solange sie die Proportion erhalten.

| Transformation | Daten        |
|----------------|--------------|
| $x$ (Original) | 10, 20, 30   |
| $x \cdot 5$    | 50, 100, 150 |
| $x - 20$       | -10, 0, 10   |

: Beispiel für proportionale Transformation

Eine dieser proportionserhaltenden Transformationen ist das Zentrieren.

### Zentrieren

Beim Zentrieren wird von jedem Wert der Mittelwert abgezogen. Das entspricht der letzten Tabellenzeile, weil 20 der Mittelwert der Zahlenreihe ist. Die transformierten Daten bilden dann die Differenz zum Mittelwert ab.

#### Formel

$$
x' = x - \bar x
$$ Sie sind dran!

::: aufgabe
**1.** Zentrieren Sie die Variable `Diameter` aus dem `trees`-Datensatz und speichern sie das Ergebnis als eine neue Spalte namens `Diameter_centered` ab.
:::

```{r center, exercise = TRUE, exercise.setup = "silentsetup", exercise.caption = "Zentrieren"}

```

```{r center-solution}
trees$Diameter_centered <- trees$Diameter - mean(trees$Diameter)
```

```{r meancenter}
question_numeric("2. Was ist der Mittelwert einer zentrierten Variable und warum ist das so? Prüfen Sie notfalls im obigen Codeblock nach.",
                 answer(0, correct = TRUE, message = "Richtig! Der Mittelwert einer zentrierten Variablen ist immer 0."))
```
</br>
<details>
<summary><a>▼ mathematischer Beweis, dass der Mittelwert einer zentrierten Variablen 0 sein muss</a></summary>

::: infobox
Es ist auch mathematisch zu zeigen, dass der Mittelwert einer zentrierten Variable immer 0 ist:

```{=tex}
\begin{align}
\mbox{Allgemeine Formel für arithmetisches Mittel} & ~ & \bar x' = \frac{\sum_{i = 1}^n x_i'}{n} &= 0 \\
\mbox{Einsetzen der Zentrierungsformel} & ~ & \frac{\sum_{i = 1}^n (x_i - \bar x)}{n} &= 0 \\
\mbox{Aufspalten der Summe in mehrere Summen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n \bar x}{n} &= 0 \\
\mbox{\(\bar x\) ist eine Konstante - Summe einer Konstanten = \(n \cdot Konstante\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \bar x}{n} &= 0 \\
\mbox{Einsetzen der Formel für \(\bar x\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \frac{\sum_{i = 1}^n x_i}{n}}{n} &= 0 \\
\mbox{Kürzen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n x_i}{n} &= 0 \\
\mbox{was zu zeigen war: 0 ist 0} & ~ & \frac{0}{n} &= 0
\end{align}
```
:::

</details>

#### Modell mit zentriertem Prädiktor

Jetzt können wir das Modell noch einmal mit der zentrierten Variable rechnen:

```{r centernew, exercise = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

```{r centerquestion}
quiz(
question_numeric("Wie lautet $b_0$ (*Intercept*) (auf drei Nachkommastellen)?",
                 answer(0.854347, correct = T),
                 answer(0.854, correct = T)),
question_numeric("Wie lautet $b_1$ (*Slope*) (auf drei Nachkommestellen)?",
                 answer(0.056476, correct = T),
                 answer(0.056, correct = T)),
caption = "3. Koeffizienten finden")
```

Am Modell hat sich nichts geändert außer dem *Intercept*.

Durch das Zentrieren haben wir 0 zu einem sinnvollen Wert gemacht, der auch tatsächlich durch die Daten abgedeckt ist.

Stellen wir das visuell dar:

Nur die x-Achse ist anders skaliert, die Proportionen sind aber erhalten.

```{r echo=FALSE, message=FALSE, warning=FALSE}
uncentered <- ggplot(trees, aes(x = Diameter, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)")

centered <- ggplot(trees, aes(x = Diameter_centered, y = Volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm), zentriert", y = "Volumen (m³)")

gridExtra::grid.arrange(uncentered, centered)
```

Bei den unzentrierten Daten ist 0 nicht im Wertebereich enthalten und lässt sich nicht sinnvoll interpretieren.

Nach dem Zentrieren ist 0 ein sinnvoller Wert, nämlich der Mittelwert!

Die Steigung $b_1$ ändert sich durch das Zentrieren nicht.

#### Interpretation zentrierte Prädiktoren

```{r questioncenter}
question_checkbox("4. Wie würden Sie $b_0$ jetzt interpretieren, wo der Prädiktor zentriert wurde?",
                  answer("Bei einem mittleren Baumdurchmesser sagt das Modell 0.854 m³ Holzernte voraus.", correct = TRUE, message = "$b_0$ ist der vorhergesagte Wert, wenn der Prädiktor den Wert 0 annimt. Da nun 0 dem Mittelwert entspricht, können wir von einem mittleren Durchmesser sprechen"),
                  answer("Wenn man das Volumen um eine Einheit erhöht, steigt der mittlere Durchmesser um 0.854 cm", correct = FALSE, message = "Hier ist alles verdreht. Zunächst, das Volumen ist unser Kriterium, also können wir daraus nicht den Durchmesser vorhersagen. Aber selbst dann wäre das Schema „Wenn man Prädiktor um 1 Einheit erhöht, um wie viel ändert sich dann die Vorhersage?“ für die Interpretation für das Regressionsgewicht $b_1$ geeignet und nicht für die Regressionskonstante $b_0$. Zuletzt: Die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also ist cm hier auch nicht die richtige Einheit."),
                  answer("Bei einem hypothetischen Durchmesser von 0.854 cm nimmt das vorhergesagte Volumen den Mittelwert an", correct = F, message = "Das stimmt leider nicht, denn die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also in diesem Fall m³. Und dann ist auch noch der Mittelwert an der falschen Stelle, nämlich eigentlich ist 0.854 das vorhergesagte Volumen in m³ bei einem Durchmesser von 0 - da wir aber zentriert haben, entspricht das einem mittleren Durchmesser.")
                  )
```

```{r importantcenter}
question_text("5. Haben Sie das Wichtigste mitgenommen? Was repräsentiert der Wert 0 bei einer zentrierten Variablen? (Stichwort)", 
            correct = "Super! Sie haben das Wichtigste verstanden.", incorrect = "Probieren Sie es nochmal (vielleicht eine andere Schreibweise). Zentrierte Daten haben als Wert ihren Abstand zum Mittelwert. Der Mittelwert hat den Abstand 0 zum Mittelwert",
                 answer("den Mittelwert", correct = T),
                 answer("arithmetisches Mittel", correct = T),
                 answer("Mittelwert", correct = T)
)
```

Sie sind schon sehr weit gekommen! Schauen wir uns jetzt mal an, was noch so im R-Output zu finden ist.

![](images/tree_dance.gif)

## Modellgüte $R^2$

Konzentrieren wir uns nun auf einen neuen Bereich im R-Output:

```{r modelgoodness, exercise = T, exercise.eval = TRUE}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

Das Bestimmtheitsmaß (auch Determinationskoeffizient) $R^2$ finden wir in der vorletzten Zeile, bei:

```         
Multiple R-squared:  0.9353   <------- R²
```
<details>
<summary><a>▼ Warum heißt es im Output „Multiple R-squared“?</a></summary>
::: infobox
Der Präfix "Multiple" ist wie eine Warnung: Wenn mehr als ein Prädiktor im Modell verwendet werden, wird $R^2$ mit jedem Prädiktor zwangsläufig höher, außer man korrigiert diese Verzerrung. Das heißt dann `Adjusted R-squared` und das ignorieren wir hier alles, weil wir nur einfache lineare Regression machen, also nur einen Prädiktor verwenden.
:::
</details>

#### Was ist $R^2$?

$R^2$ definiert, wie gut ein Modell an die Daten angepasst ist und heißt deswegen auch „Anpassungsgüte“. 

Um $R^2$ zu verstehen, ist es hilfreich zu verstehen, dass man die Varianz im Kriterium (Schwankungen im Holzvolumen) aufteilen kann in zwei Teile: 

- in einen durch das Modell (0.056 * Durchmesser - 1.04) erklärten Teil 
- und in den Fehler, der nicht erklärbare zufällige Abweichung vom Modell ist.

Aus beiden Teilen zusammen ergibt sich die Gesamtvarianz des Kriteriums. 

Die Formel für $R^2$ ist dann: 

$$
R^2 = \frac{\text{durch das Modell erklärte Varianz in y}}{\text{Gesamte Varianz in y}} = \frac{\sum_{i=1}^n( \hat y_i - \bar y)^2}{\sum_{i=1}^n(y_i - \bar y)^2}
$$

### Interpretation 

$R^2 = 0.9353$

> „93,53% der Varianz im Holzvolumen können durch das Modell aufgeklärt werden.“ 

Das Modell besteht in diesem Fall nur aus einem Prädiktor, deswegen können wir auch „Modell“ durch den Prädiktor ersetzen:

„93,53 % der Varianz im Holzvolumen können auf den Prädiktor "Durchmesser" zurückgeführt werden.“

Die Höhe des Wertes ist nicht wirklich gut vergleichbar, da es vom Forschungsgebiet abhängt, wie viel Varianzaufklärung als "gut" gilt. Über 90% Varianzaufklärung durch das Modell sind aber fast schon unglaublich und das kommt in der Praxis eher selten vor.  

### Zusammenhang mit Korrelationskoeffizient $r$

Im Fall der einfachen linearen Regression, aber nicht bei multipler Regression, entspricht $R^2$ der quadrierten Pearson-Korrelation $r$ des Prädiktors mit dem Kriterium. Daraus folgt: $\sqrt{R^2} = r$

#### Übung
::: aufgabe
Berechnen Sie den Korrelationskoeffizienten auf zwei Wegen: 

- als Wurzel von $R^2$
- mit der `cor()`-Funktion

Sind die Ergebnisse identisch?
:::

```{r cor, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Korrelation"}
r2 <- 0.9353

```

```{r cor-solution}
r2 <- 0.9353
sqrt(r2)
cor(trees$Diameter, trees$Volume) 
# 0.967109 == 0.967112
# Die kleine Abweichung ist ok, wir runden auf 4 Nachkommastellen, weil R² uns nicht präziser gegeben ist.
```

```{r korrelationsfrage}
question_numeric("Geben Sie Ihren errechneten Wert der Pearson-Korrelation zwischen Volumen und Durchmesser ein, gerundet auf 4 Nachkommastellen!",
                 answer(0.9671, correct = TRUE, message = "Wir runden auf 4 Nachkommastellen, weil R² uns nicht präziser gegeben ist."))
```

#### Reminder: Interpretation Korrelation
Wie interpretiert man noch mal den Korrelationskoeffizienten? Siehe Tutorial Korrelation.

- Stärke des Zusammenhangs (alles ab ca. |.5| ist ein starker Zusammenhang)
- Richtung des Zusammenhangs

„0.9671 ist ein starker positiver Zusammenhang“.

Nun haben Sie sich ausgiebig mit der Modellgüte befasst. Allerdings können wir damit noch keine allgemeinen Aussagen treffen über Kausalität, denn $R^2$ ist ein rein deskriptives, also *beschreibendes* Maß. Um Schlussfolgerungen abzuleiten, müssen wir ein bisschen Inferenzstatistik betreiben (schließende Statistik).

## Signifikanztests

### Test des Gesamtmodells

Die Problemstellung ist folgende: 

Alles, was wir bisher beobachtet haben, gilt zwar in unserer kleinen Stichprobe von 31 spätblühenden Traubenkirschen, aber wir wissen nicht, ob wir die Ergebnisse auch verallgemeinern können. 

Haben wir nicht vielleicht eine Stichprobe gezogen, in welcher der Durchmesser und das Holzvolumen **zufällig** zusammenhängen, obwohl das eigentlich generell im ganzen Wald nicht der Fall ist?

![](images/ftest.jpg){width=80%}

Die Nullhypothese $H_0$ des Tests lautet: $R^2$ beträgt in Wahrheit 0. 

Was der Signifikanztest tut, ist zu schauen: Wie groß ist die Wahrscheinlichkeit $p$, die vorliegende Stichprobe (oder eine noch extremere) aus einer Population zu ziehen, in der die Nullhypothese gilt?

Wie groß ist die Wahrscheinlichkeit, in den Wald zu gehen, 31 Bäume zu fällen, bei denen der Durchmesser zufällig 93% der Varianz im Volumen aufklären kann, obwohl im ganzen Wald der Durchmesser eigentlich nichts mit dem Volumen zu tun hat? 

Wenn es sich als sehr unwahrscheinlich herausstellt, gewinnen wir gewissermaßen rückwärts die Erkenntnis, dass die Nullhypothese aller Wahrscheinlichkeit nach nicht in der Population gilt.

Aber ab wann ist etwas sehr unwahrscheinlich?

Das sind arbiträr festgelegte Grenzen, die sich je nach Fachgebiet unterscheiden, aber ein allgemeiner Standard ist ein Signifikanzniveau $\alpha = 0.05\ \ (5\%)$.

#### R-Output

Wir finden den Test des Gesamtmodells in der allerletzten Zeile des R-Outputs:

```{r ftest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter_centered, data = trees)
summary(fit_centered)
```

Der Test des Gesamtmodells ist ein $F$-Test, da zwei Varianzen ins Verhältnis gesetzt werden:
Die durch das Modell erklärte Varianz, und die nicht erklärte Varianz. (Immer, wenn Varianzen verglichen werden, klingt das in Ihren Ohren nach $F$-Test. Zumindest sollte das so sein.)

Wie genau die `F-statistic` zustande kommt und was es mit den Freiheitsgraden `DF` auf sich hat, kann hier nicht ausführlich behandelt werden. Das sind nur Zwischenschritte auf dem Weg zur Berechnung des $p$-Werts. Wichtig ist, dass Sie den $p$-Wert als Endresultat interpretieren können, egal ob er von einem $F$-, $t$- oder $z$-Test ist. 

Das hier ist die relevante Zeile:

```
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16   <--- p-Wert Gesamtmodelltest
```

### Interpretation des $p$-Werts

<details>
  <summary><a>▼ Wie wird `2.2e-16` gelesen?</a></summary>
  </br>
  
::: vorteile
  Keine Panik - das ist wissenschaftliche Notation (*scientific notation*).
  
  Das wird verwendet, um sehr viele Nullen kompakt darzustellen. 
  
  So ist das Schema:
  
  `2.2e-16`$= 2.2 \cdot 10^{-16} = 0\overbrace{.0000000000000002}^{\substack{\text{Komma} \\ \text{16 Stellen nach vorne}}}2$
  
  - `e` steht für „mal 10 hoch Exponent“
  
  - `-16` ist der Exponent
  
Ein Positiv-Beispiel wäre:

`5.12e+12`$= 5.12 \cdot 10^{12} = 5\overbrace{120\ 000\ 000\ 000}^{\substack{\text{Komma} \\ \text{12 Stellen nach hinten}}}.0$
  
:::
</details>
</br>

Der $p$-Wert ist eine Antwort auf folgende Frage: „Unter der Annahme, dass $R^2$ in der Population 0 ist — Wie wahrscheinlich ist es, zufällig eine Stichprobe zu ziehen in der das vorliegende oder ein noch extremeres $R^2$ zustande kommt?“

> Angenommen, $\alpha = 0.05$:
> 
> Wenn $p \le 0.05$ -> unwahrscheinlich -> Nullhypothese verwerfen
>
> Wenn $p > 0.05$ -> wahrscheinlich -> Nullhypothese beibehalten

```{r pquestion}
learnr::question_radio("Wie würde Ihre Hypothesenentscheidung für das Beispiel aussehen? (α = 0.05)",
               answer("Nullhypothese beibehalten", message = "Da p mit 0.00…22 kleiner als 0.05 ist, sollten wir die Nullhypothese verwerfen."),
               answer("Nullhypothese verwerfen", correct = TRUE, message = "Da p unglaublich klein ist, auf jeden Fall kleiner als das Signifikanzniveau von 0.05, ist es richtig, die Nullhypothese zu verwerfen."),
               allow_retry = TRUE
)
```

#### häufige inkorrekte Formulierungen

Da wir für die Errechnung des $p$-Werts eine Bedingung annehmen, nämlich dass die Nullhypothese gilt, ist der $p$-Wert eine *bedingte* Wahrscheinlichkeit.

Aussagen wie: <s>„Mit einer Wahrscheinlichkeit von [$p$ %] gilt die Nullhypothese“</s> sind deshalb **falsch**. Ob die Nullhypothese in Wahrheit gilt oder nicht, werden wir nie herausfinden - es sei denn, wir messen die ganze Population - aber wir können schätzen, wie wahrscheinlich es ist, zufällig das vorliegende Ergebnis oder ein extremeres zu ziehen, wenn wir annehmen, dass die Nullhypothese gilt - und daraus wiederum Rückschlüsse ziehen darüber, ob wir die Nullhypothese verwerfen oder beibehalten.

### Grafische Darstellung

Da $p$ in diesem Fall unglaublich klein ist, ist es schwierig, alle relevanten Teile in einem Plot unterzubringen. Deswegen gibt es 3: 

- Übersicht (komplett herausgezoomt)
- Zoom Nr. 1 
- Zoom Nr. 2

```{r uebersichtplot, message=FALSE, warning=FALSE}

### Hilfsfunktion definieren, um Ausschnitte aus der F-Funktion zu generieren

.pshade <- function(x, min, max, df1, df2) {
    y <- df(x = x, df1 = df1, df2 = df2)
    y[x < min  |  x > max] <- NA
    return(y)
}

### Parameter definieren

alpha = 0.05
df1 = 1
df2 = 29
detail = 800  # Detailgrad des Plots
f_krit <- qf(1 - alpha, df1, df2)
f_emp <- 419.4  # aus dem R-output übernommen
xlim <- f_krit + 2

### Übersichts-Plot
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim, df1 = df1, df2 = df2),
                n = detail) +  
  geom_function(fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
  annotate("segment", x = f_emp, xend = f_emp, y = 0, yend = 0.1) +
  annotate("label", y = 0.1, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
  annotate("segment", xend = f_krit, x = 50, yend = 0, y = 0.1, 
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("label", x = 50, y = 0.1, label = "kritische\nGrenze") +
  xlim(0, f_emp + 100) +
  labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Übersicht", subtitle = "F-Verteilung") +
  theme_minimal()

```


In der Übersicht sehen Sie den empirischen (=beobachteten) F-Wert, der sich aus der Varianzaufklärung $R^2$ des Modells berechnen lässt. Er entspricht `F-statistic` im R-Output. 

Hier noch mal die relevante Zeile aus dem Output:

```
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```

<details>
  <summary><a>▼ \* Hintergrundwissen: Wie wird der $F$-Wert berechnet?</a></summary>
  </br>
  
::: infobox
\begin{align*}
F &= \frac{R^2}{1-R^2} \\
F &= \frac{\text{durch das Modell erklärte Varianz}}{\text{nicht erklärbare Varianz}}
\end{align*}
:::
</details>
</br>

Die blaue Fläche repräsentiert 95% der Fläche unter der Kurve. Nach den 95% beginnen die restlichen 5% der Fläche, in gelb, und dazwischen liegt die kritische Grenze. Ist der empirische F-Wert größer als die kritische Grenze, ist er innerhalb der gelben Fläche, also innerhalb den äußersten 5%. Ab diesem Punkt wird die Nullhypothese als "unwahrscheinlich" verworfen - der Test wird signifikant. 

```{r zoom1, message = F, warning = F}
### Zoom 1
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_krit, xlim)), y = 0.1, xend = mean(c(f_krit, xlim)), yend = 0.008,
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = f_krit, y = 0.1, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.12, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim)), y = 0.12, label = "5 % der\nFläche") +
    annotate("label", x = 1, y = 0.1, label = "95 % der\nFläche") +
    scale_x_continuous(limits = c(0, xlim)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 1",
         subtitle = "Streckung der x-Achse") +
    theme_minimal()


```


#### Wo ist der $p$-Wert?

Die Fläche unter Kurve ab dem empirischen $F$-Wert bis $+\infty$ entspricht dem $p$-Wert.

Dieser ist in diesem Fall so klein, dass man erst sehr weit hineinzoomen muss (siehe Zoom Nr. 2). Dort kann man den $p$-Wert als Fläche erkennen. (Remember, im Output stand ja bereits, $p < 2.2 \cdot 10^{-16}$, was 15 Nullen nach dem Komma entspricht - in der Übersicht überhaupt nicht sichtbar.)


```{r fplots, message=FALSE, warning=FALSE}
### Zoom 2

xlim_small <- f_emp + 100
ylim_small <- 1e-19
ggplot() +
    stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
      stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_emp, xlim_small)), yend = 0.2e-20, xend = mean(c(f_emp, xlim_small)), y = 1e-20, # p-Pfeil
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 50, yend = 0, xend = f_krit, y = 2.5e-20, # Pfeil krit. Gr.
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 2.5e-20, label = "kritische\nGrenze") +           # Label kritische Grenze
    annotate("segment", x = 50, yend = 7.5e-20, xend = 2, y = 7.5e-20, # Pfeil krit. Gr. # Pfeil 95%
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 7.5e-20, label = "95 % der\nFläche") + # Label 95%
    annotate("label", x = mean(c(f_emp, xlim_small)), y = 1.2e-20, label = "Fläche\n= p-Wert") + # p-Label
    annotate("label", x = 200, y = 5e-20, label = "5 % der\nFläche") +
    annotate("segment", x = f_emp, y = 0, yend = 5e-20, xend = f_emp) +
    annotate("label", y = 5e-20, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
    xlim(0, xlim_small) +
    coord_cartesian(ylim = c(0, ylim_small), xlim = c(0, xlim_small), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 2",
         subtitle = "Streckung der y-Achse") +
    theme_minimal()

```

#### zum Vergleich: nicht signifikant

Hier noch mal der entgegengesetzte Fall:

Falls der empirische $F$-Wert innerhalb der blauen 95% liegen würde, die als „wahrscheinlich“ gelten - wie würde unsere Hypothenentscheidung dann lauten?

Wir würden die Nullhypothese beibehalten, da $p > \alpha = 5.21 \%> 5\%$. 

Die Wahrscheinlichkeit, die beobachtete Stichprobe aus einer Population zu ziehen, in welcher die Nullhypothese gilt, ist höher als die von uns angesetzte Grenze (Signifikanzniveau), ab der etwas als „unwahrscheinlich“ gilt.

```{r notsignificantplot, message=FALSE, warning=FALSE}
# devtools::install_github("NicolasH2/ggbrace")
# WARNING! GITHUB DEPENDENCY, wird wahrscheinlich nicht automatisch gehandled - momentan habe ich das testweise verlegt auf setup chunk
# library(ggbrace)

### Parameter festlegen
f_emp <- 3.0
xlim_new <- f_krit + 1
p <- df(3, df1, df2) # 0.052
### PLOT 
### Zoom 1
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
 stat_function(geom = "area",              # p-Wert
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_new, df1 = df1, df2 = df2),
                n = detail) +  
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = f_krit, y = 0.3, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.3, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim_new)), y = 0.04, label = "α = 5 %") +
    annotate("label", x = 2.1, y = 0.05, label = "1 - α = 95 %") +
    annotate("segment", x = f_emp, y = 0, yend = .3, xend = f_emp) +
    annotate("label", y = 0.3, x = f_emp, label = "empirischer\nF-Wert") + 
    annotate("label", y = .13, x = 3.63, label = "p = 5.21 %") +
    geom_brace(aes(x = c(0, f_krit), y = c(0.0, 0.03))) +
    geom_brace(aes(x = c(f_krit, xlim_new), y = c(0.0, 0.023))) +
    geom_brace(aes(x = c(f_emp, xlim_new), y = c(0.05, 0.115)), mid = 0.29) +
    
    scale_x_continuous(limits = c(0, xlim_new)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim_new), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Beispiel",
         subtitle = "nicht signifikant (p > α)") +
    theme_minimal()

# Fail, does not render correctly in output
# library(pBrackets)
# grid.brackets(518, 453, 858, 453, h = .1)
# grid.brackets(702, 478, 858, 478, h = .09)
# grid.brackets(50, 478, 702, 487, h = .1, curvature = 0.2)


```


### Lokaltest

Der Lokaltest testet jeden Regressionskoeffizienten einzeln, und ist deswegen für die multiple lineare Regression wichtig, wo es mehrere Prädiktoren gibt. 

Im Fall der einfachen linearen Regression entspricht der Lokaltest dem Test für das Gesamtmodell, weil das gesamte Modell nur aus einem Prädiktor besteht.

Das lässt sich auch schön zeigen anhand der Beziehung zwischen der $t$-Statistik aus dem Lokaltest und der $F$-Statistik aus dem Gesamtmodelltest:

Generell gilt: $F = t^2$.

mit $t^2 = 20.48^2 = 419.4 = F$

Wo finde ich diese Werte im R-Output?

#### R-Output

Die Ergebnisse der Lokaltests befinden sich im Abschnitt `Coefficients`:

```{r localtest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(Volume ~ Diameter, data = trees)
summary(fit_centered)
```

Dabei ist wieder vor allem der $p$-Wert relevant. Er befindet sich in der Spalte `Pr(>|t|)`.

Der $p$-Wert für den Test von $b_1$ finden wir in der Zeile `Diameter`, also der Variablenname des Prädiktors.

Er ist identisch mit dem Wert aus dem Gesamtmodelltest! Auch das stützt, dass die Tests im Fall der einfachen linearen Regression äquivalent sind. 

```
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
Diameter     0.056476   0.002758   20.48  < 2e-16 *** <----- p-Wert
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Den Lokaltest für *Intercept* ignorieren wir, da es in der Regel nicht interessant ist zu wissen, ob der Achsenabschnitt in Wirklichkeit 0 ist - das wäre ja eigentlich gar kein Problem, deswegen müssen wir das auch nicht herausfinden. 

Wichtig ist, herauszufinden, ob sich das Regressionsgewicht signifikant von 0 unterscheidet. 

Da der Lokaltest nur für die multiple lineare Regression Bedeutung entfaltet, gehen wir nicht weiter im Detail darauf ein.

Sie haben jetzt hoffentlich erfolgreich verstanden:

- <input type="checkbox" unchecked> warum wir unser Modell testen
- <input type="checkbox" unchecked> was wir testen
- <input type="checkbox" unchecked> wo Sie den $p$-Wert finden, sowohl für Lokaltest als auch Gesamtmodelltest

::: aufgabe
Übungsaufgaben dazu finden Sie in einem eigenen Übungs-Kapitel, wo Sie alles Gelernte an einer Reihe von R-Outputs testen können. 
:::

Prima! Weiter geht es damit, wie Sie Daten und ein Modell visualisieren können.

![](images/tree.gif)


## Visualisierungen

Es gibt viele Wege, eine Punktwolke mit Regressionsgerade in R herzustellen.

Ich stelle hier nur zwei vor:

1.  Der schnelle Weg, mit base R
2.  Der schöne Weg, mit ggplot

Aber es ist unfassbar wichtig, mindestens einen dieser Wege zu können, um schnell und unkompliziert visuell zu überprüfen, ob die Daten überhaupt einen linearen Zusammenhang haben, und auch um zu sehen, wie gut die Gerade die Datenpunkte "erklärt".

### Der schnelle Weg mit base R

```{r, echo = TRUE}
fit <- lm(Volume ~ Diameter, data = trees) # Modell erstellen

plot(Volume ~ Diameter, data = trees) # Punktdiagramm erstellen

abline(fit) # Regressionsgerade ins Punktdiagramm einbinden
```

#### Code Breakdown

-   Modell erstellen mit `lm()` und abspeichern wie gewohnt.

-   `plot()`:

    Funktioniert genau wie `lm` mit der Formelschreibweise `y ~ x`

    (Pluspunkt für Konsistenz)

-   `abline()`:

    heißt literally $a$-$b$-Line und fügt eine Gerade zum Plot hinzu.

    $a$ ist in dem Fall Intercept und $b$ Slope.

    Das aufregende und schöne ist: Wir können einfach das gesamte gespeicherte Modell `fit` als Argument übergeben, und `abline()` weiß selbst wo die Koeffizienten zu finden sind und liest sie aus.

#### Übung

::: aufgabe
Erstellen Sie selbst ein simples Punktdiagramm mitsamt Regressionsgerade mit base R.

Damit es etwas spannender wird, verwenden wir mal einen anderen Datensatz: In `airquality` finden Sie Daten der Luftqualität in New York im Sommer 1973. Unter anderem gemessen wurde die Windgeschwindigkeit in Meilen pro Stunde, und die maximale Tagestemperatur in Grad Fahrenheit.

1.  Stellen Sie ein Regressionsmodell namens `temp_model` auf, was Veränderungen der Temperatur durch Veränderungen der Windgeschwindigkeit erklären kann.
2.  Erstellen Sie ein Punktdiagramm
3.  Fügen Sie die Regressionsgerade hinzu

Variablennamen: `Wind` = Windgeschwindigkeit `Temp` = Temperatur
:::

```{r slopeestim}
question_radio("Geben Sie einen Tipp ab: Welche Steigung wird die Regressionsgerade vermutlich haben?",
               answer("positiv", message = "Leider nicht so wahrscheinlich - dann würde es heißer werden, wenn der Wind schneller weht."),
               answer("negativ", correct = T, message = "Sinnvoll, da höhere Windgeschwindigkeit eher zu niedrigeren Temperaturen führt -> negativer Zusammenhang"))
```

```{r baser, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "base R Grafik"}

```

```{r baser-solution}
temp_model <- lm(Temp ~ Wind, data = airquality) # Modell erstellen

plot(Temp ~ Wind, data = airquality) # Punktdiagramm erstellen

abline(temp_model) # Regressionsgerade zum Plot hinzufügen
```

### Der schöne Weg mit ggplot

```{r, echo = T}
library(ggplot2) # Paket laden

ggplot(trees, aes(x = Diameter, y = Volume)) +   # Daten und Mappings definieren
  geom_point() +                                 # Punktdiagramm
  geom_smooth(method = "lm", se = F) +           # Regressionsgerade
  theme_minimal() +                              # Schnick-Schnack
  labs(x = "Durchmesser (cm)", y = "Volumen (m³)", title = "spätblühende Traubenkirsche")
```

#### Code Breakdown

Ich setze hier Grundkenntnisse in ggplot voraus, da das den Rahmen sprengen würde (siehe Tutorial zur Visualisierung).

-   `geom_smooth()` ist das Geom, was wir verwenden um eine Regressionsgerade zu zeichnen. Der entscheidende Unterschied zu base R ist, dass wir kein Modell aufstellen müssen vorher, sondern das wird automatisch von `geom_smooth()` intern übernommen.

    -   `method = "lm"`: Da es auch viele andere mögliche Modelle gibt, müssen wir im Argument `method` angeben, dass wir die Funktion `lm` nutzen wollen, also ein **l**ineares **M**odell.

        Intern wird dadurch `lm()` aufgerufen, standardmäßig mit der Formel `y ~ x`. Das Ergebnis hängt davon ab, welche Variablen wir auf $x$ mappen und auf $y$. Dieser Sachverhalt wird uns auch in einer neutralen Nachricht bewusst gemacht: `geom_smooth() using formula = 'y ~ x'`.

    -   `se = F`: **s**tandard **e**rror = **F**ALSE. Standardmäßig ist dieses Argument `TRUE`, und es wird automatisch ein Konfidenzintervall mit eingezeichnet, was wir aber nicht benötigen im Moment.

#### Übung

::: aufgabe
Erstellen Sie nun analog zur letzten Übung eine Grafik mittels ggplot, die den Einfluss von Wind auf die Temperatur in einem Punktdiagramm mit Regressionsgerade darstellt.

Die Daten befinden sich wieder im `airquality`-Datensatz.
:::

```{r airggplot, exercise = TRUE, exercise.cap = "ggplot Grafik"}

```

```{r airggplot-solution}
library(ggplot2)

ggplot(airquality, aes(x = Wind, y = Temp)) +  # Datensatz und Mappings definieren
  geom_point() +                          # Punktdiagramm zeichnen
  geom_smooth(method = "lm", se = F) +    # Regressionsgerade zeichnen
  theme_minimal()                         # weißer Hintergrund

# Zusatzaufgabe:
                     
#  geom_smooth(method = "lm", se = F, color = "red") + 
```

::: aufgabe
**\* Zusatzaufgabe**

Ändern sie die Farbe der Regressionsgeraden!
:::

Super, jetzt haben Sie gelernt, wie eine einfache lineare Regression visuell dargestellt werden kann!

Im nächsten Kapitel werden wir uns mit den Vorannahmen beschäftigen, die erfüllt sein müssen, damit die lineare Regression überhaupt zulässig ist.

## Vorraussetzungen prüfen

Hier ein kleiner Überblick, darüber, wie die Voraussetzungen geprüft werden können in R:

+----------------------------------------+-----------------------------------+
| Vorannahme                             | Überprüfung                       |
+========================================+===================================+
| 1.  metrische Variablen                | Vorwissen über die Erhebung       |
+----------------------------------------+-----------------------------------+
| 2.  Zufällige Stichprobe               | Vorwissen über die Erhebung       |
+----------------------------------------+-----------------------------------+
| 3.  Linearer Zusammenhang in den Daten | Streudiagramm                     |
+----------------------------------------+-----------------------------------+
| 4.  Es gibt Variation beim Prädiktor   | Streudiagramm                     |
+----------------------------------------+-----------------------------------+
| 5.  Homoskedastizität                  | `plot(fit)`                       |
+----------------------------------------+-----------------------------------+

: Vor dem Modellieren

+----------------------------------------------+-----------------------------------------------+
| Vorannahme                                   | Überprüfung                                   |
+==============================================+===============================================+
| 6.  Unkorreliertheit der Residuen mit der UV | `plot(fit)`                                   |
+----------------------------------------------+-----------------------------------------------+
| 7.  keine Autokorrelation der Residuen       | `car::durbinWatsonTest()`                     |
+----------------------------------------------+-----------------------------------------------+
| 8.  Normalverteilung der Residuen            | -   Q-Q-Plot: der zweite Plot bei `plot(fit)` |
|                                              |                                               |
|                                              | -   Histogramm der standardisierten Residuen  |
|                                              |                                               |
|                                              |     -   `library(tidyverse)`                  |
|                                              |                                               |
|                                              |     -   `resid(fit) %>% scale() %>% hist()`   |
+----------------------------------------------+-----------------------------------------------+

: Nach dem Modellieren

### 1. Metrische Variablen

Sowohl der Prädiktor als auch das Kriterium müssen metrisch sein.

<details>
<summary><a>▼ \* Hier sind einige andere Verfahren, falls das nicht gegeben ist:
</a></summary>
::: infobox

kategorialer Prädiktor + metrisches Kriterium

-   bei zwei Kategorien / Gruppen: $t$-Test

-   bei mehr als zwei Kategorien / Gruppen: Varianzanalyse (ANOVA) oder generalisiertes lineares Modell (GLM) mit Dummy-Kodierung

metrischer Prädiktor + kategoriales Kriterium:

-   logistische Regression

:::
</details>
</br>

### 2. Zufällige Stichprobe

Im Beispiel mit der spätblühenden Traubenkirsche wäre es zum Beispiel wichtig, dass nicht selektiv nur Bäume vermessen wurden, die eine besonders gute Wuchsform hatten, sondern eine ausreichend große, zufällige Auswahl aus allen möglichen Bäumen getroffen wurde, damit eine Normalverteilung angenommen werden kann. Dabei heißt ausreichend große Stichprobe als Faustregel ungefähr $n > 30$.

### 3. Linearer Zusammenhang in den Daten

Manche Daten werden besser nicht durch Geraden erklärt, sondern haben vielleicht quadratische Zusammenhänge, wie in diesem Beispiel:

```{r}
data <- data.frame(Temperatur=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60),
                   Enzymaktivität=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))


straight_fit <- lm(Enzymaktivität ~ Temperatur, data = data)
data$Temperatur2 <- data$Temperatur^2
quad_fit <- lm(Enzymaktivität ~ Temperatur + Temperatur2, data = data)
zeit <- seq(0, 60, 0.1)
prediction <- predict(quad_fit, list(Temperatur = zeit, Temperatur2 = zeit^2))

plot(Enzymaktivität ~ Temperatur, data = data)
abline(straight_fit, col = "red")
lines(zeit, prediction, col = "blue")
```

Die Gerade erklärt die Daten nicht gut und trifft falsche Vorhersagen, während die quadratische Funktion sehr nah an der Realität liegt. (Fiktive Daten)

Die Überprüfung einer linearen Beziehung in den Daten erfolgt visuell über ein Punktdiagramm. Die Punkte sollten sich alle durch eine Gerade beschreiben lassen und keine *systematischen* Abweichungen von der Linearität haben. Zufällige Streuung hingegen ist komplett in Ordnung. 

Beispiele:

```{r plotmaking, fig.height=5, fig.width=6}
p1 <- ggplot(rtutorials::concrete, aes(x = coarse_aggregate, y = compressive_strength)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "1")

p2 <- ggplot(rtutorials::heated, aes(x = edvisits, y = maxtemp)) +
  geom_point(alpha = 0.18) +
  theme_void() +
  labs(title = "2")

p3 <- ggplot(rtutorials::airfoil, aes(x = angle, y = displacement_thickness)) +
  geom_point(alpha = 0.03) +
  theme_void() +
  labs(title = "3")

p4 <- ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "4")

p5 <- ggplot(women, aes(x = height, y = weight)) +
  geom_point(alpha = 0.5) +
  theme_void() +
  labs(title = "5")
  
p6 <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
  geom_point(alpha = 0.34) +
  theme_void() +
  labs(title = "6")

gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6)
```

```{r plotquiz}
question_checkbox("Bei welchen dieser Plots würden Sie eine zugrunde liegende lineare Beziehung vermuten?",
                  answer("1", message = "(1) Hier ist es schwer zu sagen - die Daten enthalten sehr viel Streuung. Aufällig ist die Konzentration viele Punkte auf bestimmten x-Werten."),
                  answer("2", message = "(2) Diese Beziehung zwischen Notaufnahmebesuchen (y) und Hitze (x) folgt eher einer Kurve als einer Gerade"),
                  answer("3", message = "(3) Hier liegt scheinbar eher eine Exponentialfunktion zu Grunde, keine Gerade"),
                  answer("4", correct = T, message = "(4) Hier könnte eine Gerade die Punkte gut beschreiben."),
                  answer("5", correct = T, message = "(5) Es handelt sich um den Zusammenhang zwischen Körpergröße und Gewicht."),
                  answer("6", correct = T, message = "(6) Auch hier lässt sich eine lineare Beziehung vermuten."),
                  allow_retry = TRUE
         )
```

### 4. Variation des Prädiktors

Wie sieht ein Plot ohne Variation des Prädiktors aus?

Alle Punkte liegen auf dem selben $x$-Wert. In diesem Fall hilft $x$ überhaupt nicht, Varianz in $y$ zu erklären. Die beste Vorhersage für $y$ ist dann der Mittelwert $\bar y$.

Dieser Fall ist in der Praxis extrem unwahrscheinlich, da es meistens ein bisschen zufällige Variation gibt.

```{r novariation, fig.height=5, fig.width=5}
plot(x = rep(20, 100), y = rnorm(100), yaxt = "n", xaxt = "n", xlab = "x", ylab = "y")
```

::: aufgabe
**\* Profi-Frage**
:::

```{r proquestion}
question_numeric("Welchen Wert würde $R^2$ im obigen Plot annehmen und warum?",
                 answer(0, correct = T),
                 allow_retry = T)
```

<details>
<summary><a>▼ Erklärung</a></summary>

::: infobox
\begin{align}
R^2 &= \frac{\text{Durch das Modell erklärte Varianz in y}}{\text{Gesamtvarianz in y}} \\
R^2 &= \frac{0}{\text{Gesamtvarianz in y}} = 0
\end{align}
:::
</details>


### 5. Homoskedastizität

```{r homoskedasis_grafiken, message=FALSE, warning=FALSE}
# H1 Homosked.
set.seed(1)
x <- 1:1000
y <- rnorm(1000, sd = 100)
df1 <- data.frame(x, y)
h1 <- ggplot(df1, aes(x, y)) +
  geom_point(alpha = 0.34) +
#  geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "1")

y <- 2 + 0.5 * x + rnorm(1000, sd = x)
df2 <- data.frame(x, y)
h2 <- ggplot(df2, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "2")

sd3 <- sin(seq(0, 5, length.out = 1000))^2
y <- rnorm(1000, sd = sd3)
df3 <- data.frame(x, y)
h3 <- ggplot(df3, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "3")

y <- sin(seq(0, 10, length.out = 1000)) + rnorm(1000, sd = 0.5)
df4 <- data.frame(x, y)
h4 <- ggplot(df4, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "4")

y <- 2 + 0.5 * x + rnorm(1000, sd = 30)
df5 <- data.frame(x, y)
h5 <- ggplot(df5, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "5")

y <- 10 - 0.5 * x + rnorm(1000, sd = 1000:1)
df6 <- data.frame(x, y)
h6 <- ggplot(df6, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "6")

gridExtra::grid.arrange(h1, h2, h3, h4, h5, h6)
```


```{r homoskedasis_question}
question_checkbox("Welche der Grafiken zeigen eine homoskedastische Verteilung?",
                  answer(1, correct = T, message = "(1) Hier ist es offensichtlich"),
                  answer(2, message = "(2) Das klassische Beispiel für Heteroskedastizität - die Trompetenform"),
                  answer(3), message = "(3) könnte ein Beispiel für saisonale Schwankungen sein - heteroskedastisch",
                  answer(4, correct = T, message = "(4) ist eigentlich homoskedastisch, weil die Streuung überall gleich ist, aber es liegt eine Sinuskurve anstelle einer Gerade zugrunde. (Dadurch ist dann auf jeden Fall eine andere Voraussetzung verletzt, nämlich die Linearitätsannahme)"),
                  answer(5, correct = T, message = "(5) hat eine in der Praxis fast unrealistisch kleine Streuung, aber sie ist überall gleich breit - homoskedastisch"),
                  answer(6, message = "(6) Das klassische Beispiel für Heteroskedastizität - die Trompetenform"), 
                  allow_retry = T)
```

Diese ganzen Betrachtungen kann man vor der Analyse durchführen.
Nach der Analyse können noch die Residuen überprüft werden: 

### 6. Unkorreliertheit der Residuen mit Kriterium

Werden die Residuen zum Beispiel größer, je größer das Kriterium wird, dann ist das ein Hinweis auf Heteroskedastizität. Vergleichen wir dafür noch einmal zwei Plots: 

```{r}
h1 + geom_smooth(method = "lm", se = F)
fit1 <- lm(y ~ x, data = df1)
plot(fit1, which = 1)

h6 + geom_smooth(method = "lm", se = F)
fit6 <- lm(y ~ x, data = df6)

plot(fit6, which = 1)
```


### 7. keine Auto-Korrelation der Residuen

Was bedeutet Auto-Korrelation? 

Die altgriechische Vorsilbe „Auto-“ bedeutet „Selbst-“ oder „Eigen-“. Es geht also um Korrelation der Werte mit sich selbst. 

Dabei wissen wir ja eigentlich, dass die Korrelation jeder Variable mit sich selbst perfekt ist, also 1 beträgt. Darum geht es also nicht. 

Die Frage hier ist, ob die Höhe eines Residuums die Höhe der benachbarten Residuen beeinflusst. Wenn das übermäßig der Fall ist, sind die Residuen nicht zufällig. 

Warum ist das wichtig? Das Modell baut darauf, dass ein linearer Zusammenhang zugrunde liegt, und dass jegliche Streuung in den Daten um diese Gerade rein zufälliger Natur ist, also keine Systematik dahinter steht. 

#### Überprüfung in R

Die Auto-Korrelation kann in R berechnet und ausgegeben werden mitsamt eines Hypothesentests, der die Nullhypothese $\rho = 0$ testet („Die Auto-Korrelation beträgt in Wahrheit 0“). 

```{r autocor-question}
question_checkbox("Wie müsste der p-Wert aussehen, wenn unsere Daten vermutlich aus einer Population ohne Autokorrelation stammen?",
                  answer("$p \\le 0.05$", message = "Wenn p kleiner als 0.05 wäre, müssten wir die Nullhypothese verwerfen. Da die Nullhypothese aber enthält, dass die Autokorrelation in Wahrheit 0 ist, wäre das also nicht der erwünschte Fall, diese Annahme zu verwerfen - das wäre dann ein Hinweis darauf, dass es Autokorrelation gibt."),
                  answer("$p > 0.05$", correct = T, message = "Genau! In diesem Fall halten wir danach Ausschau, dass der Test nicht signifikant wird, der p-Wert größer als 0.05 bleibt, damit die Nullhypothese beibehalten werden darf, da das das „erwünschte“ Ergebnis ist."),
                  allow_retry = T)
```

Geprüft wird das mit einem Durbin-Watson-Test. Das Paket `car` stellt eine Funktion dafür bereit:

Testen wir das zunächst an einem extremen Beispiel, was ich simuliert habe und definitiv Auto-Korrelation enthalten sollte:
Plot 4 aus dem Homoskedastizität-Abschnitt, die Residuen variieren systematisch! 

Wenn Auto-Korrelation besteht, dann variieren benachbarte Residuen gemeinsam.

```{r message=FALSE, warning=FALSE}
fit4 <- lm(y ~ x, data = df4)
pre4 <- predict(fit4)
h4 + geom_smooth(method = "lm", se = F) +
  geom_linerange(aes(ymin = pre4, ymax = y), color = "red", alpha = .4)

```

```{r, echo = TRUE}
car::durbinWatsonTest(fit4)
```

<details>
<summary><a>▼ Wie eine Auto-Korrelation berechnet wird</a></summary>

```{r, echo = TRUE}

```

</details>




### 8. Normalverteilung der Residuen

Eine letzte und zentrale Annahme ist, dass die Residuen normalverteilt sein müssen. 

Eine Möglichkeit, das grob visuell zu überprüfen, ist ein Histogramm zu erstellen der standardisierten Residuen. 

Wir erinnern uns: Ein Histogramm ermöglicht, die Verteilung einer metrischen Variable zu visualisieren. 

```{r}
resid(fit) |> scale() |> hist()
```

#### visuelle Überprüfung

Es mag willkürlich erscheinen, rein aus dem Visuellen grob zu bestimmen, ob etwas normalverteilt ist oder nicht. Mit diesem kleinen Spiel können Sie Ihre Intuition etwas schärfen:

```{r ui_distribution-guesser}
verteilungen <- c("Normal", "Uniform", "Bimodal")

fluidPage(

  shinyFeedback::useShinyFeedback(),
  shinyjs::useShinyjs(),

  # Application title
  titlePanel("Verteilungen erraten"),


  sidebarLayout(
    sidebarPanel(
      sliderInput("n", "Stichprobengröße (aka Schwierigkeitsgrad)",
                  value = 50, min = 30, max = 300, step = 1),
      # sliderInput("bins", "Anzahl Bins", min = 3, value = 20, max = 80, step = 1),
    ),

    mainPanel(
      plotOutput("plot"),
      radioButtons("radio",
                   "Aus welcher Verteilung wurde die Stichprobe gezogen?",
                   verteilungen, selected = character(0)),
      actionButton("button", "Nächster Plot"),
      tableOutput("table"),
      textOutput("text"),
      actionButton("reset", "Neustart")
    )
  )
)

```

```{r server_distribution-guesser, context = "server"}
# Die App wird separat entwickelt, da sonst für jeden Test das Laden des gesamten tutorials nötig ist
# source of truth: https://github.com/luk-brue/normal-distribution-guesser
library(shinyFeedback)
library(shinyjs)

verteilungen <- c("Normal", "Uniform", "Bimodal")

  sampler <- function() sample(verteilungen, size = 1)
  zufall <- reactiveValues(sample = sampler())
  counter <- reactiveValues(richtig = 0, falsch = 0, round = 0)

  observeEvent(input$button, counter$round <- counter$round + 1)

  observeEvent(input$n, {
    feedback(inputId = "n", text = "unmöglich", color = "darkred",
             show = {input$n < 30})
    feedback(inputId = "n", text = "schwer", color = "red",
             show = {input$n > 30 && input$n <= 39})
    feedback(inputId = "n", text = "mittelschwer", color = "orange",
             show = {input$n > 39 && input$n <= 50})
    feedback(inputId = "n", text = "medium", color = "yellow",
             show = {input$n > 50 && input$n <= 100})
    feedback(inputId = "n", text = "leicht", color = "darkgreen",
             show = {input$n > 100 && input$n <= 200})
    feedback(inputId = "n", text = "sehr leicht", color = "lightgreen",
             show = {input$n > 200})

  })

  observeEvent(input$reset, {
    zufall$sample <- sampler()
    counter$richtig <- 0
    counter$falsch <- 0
    counter$round <- 0
  })

  output$text <- renderText({

    paste("Erfolgsquote:",
          if(counter$round >= 5){
            paste(
              round(
                counter$richtig / (counter$richtig + counter$falsch) * 100,
                digits = 2
              ),
              "%")
          } else {
            "wird erst ab 5 Versuchen angezeigt"
          }
    )
  })

  output$plot <- renderPlot({
    input$button
    x <- switch(zufall$sample,
                "Normal" = rnorm(input$n),
                "Uniform" = runif(input$n),
                "Bimodal" = c(rnorm(input$n / 2), (rnorm(input$n / 2) + 5))
    )
    df <- data.frame(x)
    bins <- as.integer(nclass.Sturges(x) * 1.4)
    ggplot(df, aes(x = x)) +
      geom_histogram(bins = bins, fill = "gray", color = "black") +
      theme_minimal()
  })
  observeEvent(input$radio,
               {
                 req(input$radio)
                 test <- input$radio == zufall$sample
                 if(test){
                   counter$richtig <- counter$richtig + 1
                   showToast(type = "success",
                             message = "",
                             keepVisible = TRUE)
                 } else {
                   counter$falsch <- counter$falsch + 1
                   showToast(type = "error", message = zufall$sample,
                             keepVisible = TRUE)
                 }
                 disable("radio")
               })
  observeEvent(input$button,
               {updateRadioButtons(session, inputId = "radio",
                                   selected = character(0))
                 zufall$sample <- sampler() # Zufallsgenerator
                 enable("radio")
                 hideToast(animate = F)
               })

  output$table <- renderTable({
    data.frame(richtig = counter$richtig, falsch = counter$falsch)
  }, digits = 0)
```


## Ausblick: Multiple lineare Regression

In der Praxis wird selten nur eine einfache (im Sinne von 1-fach) lineare Regression gerechnet, sondern mehrere Prädiktoren werden verwendet, um ein Kriterium vorherzusagen oder zu erklären.

Bei unserem Forst-Beispiel ist das ja auch so: Die Höhe spielt natürlich eine wichtige Rolle, und ist nicht zu vernachlässigen, wenn wir einen guten Schätzer für das Holzvolumen eines Baumes haben wollen.

```{r}
trees$Height <- trees$Height * 0.3048  # Umrechnung Fuß in Meter

multiple_fit <- lm(Volume ~ Diameter + Height, data = trees)
summary(multiple_fit)
anova(fit, multiple_fit)
```

## Übungskapitel

Hier finden Sie eine Reihe von Aufgaben zu R-Outputs, an denen Sie das Interpretieren in Gänze üben können. 

Sie können für Übungszwecke annehmen, dass alle Voraussetzungen erfüllt sind.

::: aufgabe
**1.**
Erklärung der Variablen:

- `mpg` - Miles per Gallon, Reichweite von Autos pro Gallone Kraftstoff (höherer Wert = sparsamer!)
- `disp` - Displacement - Hubraum in cubic inches


:::
</br>

```{r ueb1_routput, exercise = TRUE, exercise.cap = "R Output"}
lm(mpg ~ disp, data = mtcars) |> summary()
```

```{r ueb_1a}
question_text("Geben Sie eine inhaltliche Interpretation der beiden Regressionskoeffizienten!",
              answer_fn(~ correct()),
              placeholder = "Eine Musterlösung erscheint, wenn Sie die Antwort einreichen, bitte vergleichen Sie selbstständig.",
              correct = "Musterlösung: \n
b0: Für einen Hubraum von 0 cubic inches sagt das Modell 29.600 Meilen pro Gallone Treibstoff vorher. Das ist inhaltlich natürlich sinnlos, aber technisch ist das die richtige Interpretation. \n
b1: Wenn man den Hubraum um einen cubic inch erhöht, verringert sich die vorhergesagte Reichweite pro Gallone um -0.041 Meilen.")
```
```{r ueb_1b}
question_text("Interpretieren Sie die Modellgüte!",
  answer_fn(~ correct()),
              correct = "Musterlösung: \n
R² beträgt 0.7183. Das bedeutet, das Modell erklärt 71,83% der totalen Varianz der Reichweiten. Das ist eine vergleichsweise hohe Varianzaufklärung.")
```
```{r ueb_1c}
question_text("Könnten wir die Ergebnisse auf eine größere Population übertragen? Interpretieren Sie die Ergebnisse des Lokal- und des Gesamtmodelltests. Welche Hypothesen werden jeweils getestet, und wie fällt Ihre Hypothesenentscheidung aus bei einem Signifikanzniveau von $\\alpha$ = 5%?",
  answer_fn(~ correct()),
              correct = "Musterlösung: \n
Der Lokaltest testet die Nullhypothese, dass das Regressionsgewicht „Hubraum“ in Wahrheit 0 beträgt. Diese Hypothese verwerfe ich, da der p-Wert kleiner als α ist. Das bedeutet, wir können davon ausgehen, dass die Stichprobe aus einer Population stammt, in der „Hubraum“ einen Einfluss auf die Reichweite hat. \n
Analog dazu testet der Gesamtmodelltest die Nullhypothese, dass das Modell in der Population in Wahrheit keine Varianzaufklärung bietet. Da der p-Wert der gleiche ist, treffe ich hier die gleiche Hypothesenentscheidung und verwerfe die Nullhypothese, und gehe davon aus, dass das Modell auch in Wahrheit eine Varianzaufklärung verschieden von 0 hat.")
```

## Abschlussquiz

## Learnings

## Abstellgleis

::: aufgabe
Wie hoch ist $e_{20}$ (das Residuum von Messung 20) beim Eingangsbeispiel im `trees` Datensatz?

Berechnen Sie mit R!

$$
e_{20} = y_{20} - \hat y_{20}
$$

:::

```{r residuumberechnen, exercise = TRUE, exercise.cap = "Residuum"}
e20 <- trees$Volume[20] - predict(fit)[20]
e20
resid(fit)[20]

```
