---
title: "Regression"
output:
  learnr::tutorial:
    language: de
    css: css/boxes.css
    fig_caption: no
runtime: shiny_prerendered
bibliography: ref.json
link-citations: TRUE
description: Einf√ºhrung in die einfache lineare Regression.
resource_files:
- css/boxes.css
---

```{r setup, include=FALSE}
library(learnr)
library(ggplot2)
library(shiny)
library(pander)
library(correlation)

if(!require(ggbrace, quietly = TRUE)){
   message("\nVersuche das Paket `ggbrace` von GitHub zu installieren...\nEs wird f√ºr eine Grafik ben√∂tigt, die ansonsten nicht funktioniert.\nF√ºr die Installation braucht es eine Internetverbindung.\n")
  try(devtools::install_github("NicolasH2/ggbrace"))
  require(ggbrace)
}else{
 library(ggbrace)
}
knitr::opts_chunk$set(echo = FALSE)
```

## Inhalt

In diesem sehr umfangreichen Tutorial wirst du die Grundlagen von
Regressionen verstehen, die Voraussetzungen testen und die Berechnung
und Interpretation √ºben.

Die Kapitel, die mit "Ex:" beginnen, bieten dabei eine interaktive Vermittlung der
Grundlagen. Wenn du diese bereits verinnerlicht hast kannst du diese auch √ºberspringen
(auf eigene Gefahr üòâ).

In unserem wissenschaftlichen Prozess sind wir weiterhin bei der
Auswertung und dem Berichten unserer Ergebnisse:

![](images/prozess.png){width="90%"}

## Lernziele

Am Ende wirst du folgendes gelernt haben:

-   <input type="checkbox" unchecked> Grundlagen der Regression </input>
-   <input type="checkbox" unchecked> Voraussetzungen der Regression zu
    pr√ºfen </input>
-   <input type="checkbox" unchecked> Modelg√ºte zu sch√§tzen </input>
-   <input type="checkbox" unchecked> Regressionsergebnisse zu
    interpretieren </input>

## Unser Beispiel

Als F√∂rster\*in in den USA stehst du vor einer sp√§tbl√ºhenden
Traubenkirsche (*Prunus serotina*)
([wiki](https://de.wikipedia.org/wiki/Sp%C3%A4tbl%C3%BChende_Traubenkirsche)).
Sie soll bald gef√§llt werden, denn sie hat ein begehrtes Holz, und du
brauchst Geld. Am liebsten m√∂chtest wissen, bevor du den Baum f√§llst,
wie viele Kubikmeter Holz wohl dabei herauskommen werden, die du
verkaufen kannst. Aber du m√∂chtest keine allzu komplexen Messungen
vornehmen. Den besten und einfachsten Sch√§tzer, den wir bequem nehmen
k√∂nnen ist der Durchmesser des Baumes, der sich ungef√§hr aus dem Umfang
errechnen l√§sst. Daf√ºr braucht es lediglich ein Ma√üband und $\pi$.

Der Baum, vor dem wir stehen, hat einen Durchmesser von 45 cm. Wie viele
Kubikmeter Holz k√∂nnen wir erwarten?

Bild: Kein Kirschbaum, aber zeigt das Prinzip.

![](images/tree_patrick.jpg){width="60%"}

[Source: US Army Corps of Engineers. Patrick Bloodgood, photographer.
[CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia
Commons]

::: infobox
Achtung! **Vereinfachungsalarm**

Wir lassen hier einige Dinge der Einfachheit halber komplett au√üer Acht,
zum Beispiel:

-   in der Forstwirtschaft wird zur Sch√§tzung des Volumens eine etwas
    kompliziertere Formel verwendet, in die neben dem Durchmesser auch
    die H√∂he mit einflie√üt
-   Dabei gibt es verschiedene Volumenma√üe f√ºr Holz mit und ohne Rinde
-   ...
:::

### Beispieldaten

Wir haben bereits Daten von 31 gef√§llten B√§umen der gleichen Art, und
k√∂nnen damit eine Vorhersage treffen. Die Daten befinden sich im
`trees`-Datensatz, der in *Base R* eingebaut ist.

Schauen wir uns die Daten mal an:

::: aufgabe
**1.** Wie sehen die ersten Zeilen des Datensatzes `trees` aus? Lass dir
den Datensatz mit `head()` anzeigen
:::

```{r head, exercise = TRUE}

```

```{r head-solution}
head(trees)
```

### Ein bisschen Data-Cleaning

::: aufgabe
**2.**

Die H√∂he `Height` lassen wir heute au√üen vor.

Der Durchmesser hei√üt im Datensatz f√§lschlicherweise `Girth`, was Umfang
bedeutet. (Das ist einfach ein Fehler in der Benennung, siehe `?trees`).

Behebe das, in dem du in `trees` mittels Assignments eine neue Variable
im dataframe `trees` namens `diameter` erstellst, die den Inhalt der
Variable `Girth` enth√§lt.
:::

```{r girth, exercise = TRUE}

```

```{r girth-solution}
trees$Diameter <- trees$Girth
```

::: aufgabe
**3.**

Die Variablen sind (typisch USA) alle in nicht-metrischen Einheiten
angegeben.

Rechne `Diameter` und `Volume` in metrische Einheiten um, damit das
Beispiel intuitiver verst√§ndlich ist. Benenne die neuen Variablen dabei
mit kleinen Anfangsbuchstaben.

`diameter` = $cm = Inch * 2.54$

`volume` = $m^3 = ft^3 * 0.0283168466$
:::

```{r feet-setup}
trees$diameter <- trees$Girth
```

```{r feet, exercise = TRUE}

```

```{r feet-hint}
# passe den Code an:
trees$variable <- trees$Variable * Umrechnungsfaktor
trees$variable <- trees$Variable * Umrechnungsfaktor

```

```{r feet-solution}
trees$diameter <- trees$Diameter * 2.54 # Inch in cm
trees$volume <- trees$Volume * 0.0283168466 #  ft¬≥ in m¬≥

```

```{r silentsetup}
# Hier drauf sollten sich alle Exercise Code Chunks, die das trees-Dataset verwenden, beziehen

trees$diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$volume <- datasets::trees$Volume * 0.0283168466 #  ft¬≥ in m¬≥

trees$diameter_centered <- trees$diameter - mean(trees$diameter)

fit <- lm(volume ~ diameter, data = trees)

trees_predict <- trees["volume"]
trees_predict$predict <- predict(fit)
```

```{r silentsetup-global}
# Und das gleiche noch mal f√ºrs Global Environment

trees$diameter <- datasets::trees$Girth * 2.54 # Inch in cm
trees$volume <- datasets::trees$Volume * 0.0283168466 #  ft¬≥ in m¬≥

trees$diameter_centered <- trees$diameter - mean(trees$diameter)
```

### Visualisierung

Ein schneller Scatterplot gibt uns jetzt Auskunft √ºber die Beziehung
zwischen Durchmesser und Volumen:

```{r scat, echo = TRUE}
ggplot(trees, aes(x = diameter, y = volume)) +
  geom_point() +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)")
```

Grunds√§tzlich sieht es so aus, dass wir mehr Holz ernten, je dicker der
Baum war.

Durchmesser und Volumen h√§ngen also irgendwie proportional zusammen. F√ºr
unsere Vorhersage w√ºnschen wir uns aber zu wissen, wie genau diese
Proportion aussieht.

Gl√ºcklicherweise sieht der Zusammenhang linear aus, das hei√üt, wir
k√∂nnten ihn sinnvoll mit einer Gerade beschreiben:

```{r echo=F, message=FALSE, warning=FALSE}
ggplot(trees, aes(x = diameter, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)")
```

Die blaue Gerade ist unser Modell, die Punkte sind die (Daten der) Realit√§t, aus der
wir das Modell abgeleitet haben. Ein Modell wird den Komplexit√§ten der
Realit√§t nie komplett gerecht. Die Gerade hier ist ein sehr einfaches Modell und beschreibt
die Realit√§t trotzdem relativ gut.

Geraden werden beschrieben durch lineare Funktionen. Unser Modell
besteht also essentiell aus einer linearen Funktion! Wie genau das alles
funktioniert, erf√§hrst du im n√§chsten Abschnitt. Vorher aber noch ein
kleiner Test, ob du auch schon mit dem Modell umgehen kannst und die
Eingangsfrage l√∂sen k√∂nntest:

```{r first}
question_numeric(
  "Wie viel Kubikmeter Holz sind zu erwarten laut der blauen Gerade (unserem linearen Modell) bei einem Durchmesser von 45 cm?",
  answer(1.5, correct = T)
)
```

Super, das waren die ersten Schritte! Im n√§chsten Kapitel werden die
mathematischen Grundlagen aufgefrischt.

![Yay!](./images/shuffling_tree.gif)

## Ex: lineare Funktionen

Bisher sieht unser Modell so aus:

![](./images/function.gif)

Wir wissen nicht genau, was was das Modell im Inneren macht, nur das wir den **Pr√§diktor**
$x$ (Durchmesser) reingeben und daraus das **Kriterium** $y$ (Volumen)
herausbekommen wollen. In diesem Kapitel erkl√§ren wir, was im Modell
vorgeht - n√§mlich eine lineare Funktion.

### ein bisschen Begriffsarbeit

Die lineare Funktion kennst du sicher noch aus der Schule:

$$
y = mx + n
$$

oder in anderer Notation:

$$
y = a + bx
$$

Egal welche der Notationen du kennst, oder auch wenn du noch gar keine
Ber√ºhrungspunkte mit linearen Funktionen hattest, der Aufbau ist immer
gleich.

Wir werden diese Notation verwenden:

$$
\hat y_i = b_0 + b_1\cdot x_i
$$

-   Dabei steht $\hat y_i$ f√ºr den durch das Modell vorhergesagten Wert, die **abh√§ngige Variable** (AV) auch engl. "*response variable*" genannt, "*output variable*" oder
    **Kriterium**. Im Beispiel w√§re es das Holzvolumen in m¬≥.

    Ein Dach √ºber der Variable kennzeichnet immer, dass es sich hier um
    einen gesch√§tzten Wert handelt.

-   $x_i$ ist der **Pr√§diktor**, die **unabh√§ngige Variable** (UV) oder engl. 
*input variable*, im Beispiel der Baumdurchmesser in cm.

-   Der tiefgestellte Index $i$ bezeichnet die Nummer eines
    Messwertpaars

Daraus ergibt sich f√ºr uns folgendes Modell:

$$
\hat{Volumen_i} = b_0 + b_1 \cdot Durchmesser_i 
$$
</br>

#### Was sind $b_0$ und $b_1$?

Beides sind Regressions**koeffizienten** - zwei Parameter, die die Lage der
Gerade beschreiben.

-   $b_0$ ist die **Regressions*konstante***.

    Das ist der vorhergesagte Wert, wenn $x$ 0 ist. Das ergibt sich aus
    der Gleichung, wenn man f√ºr $x$ 0 einsetzt:

    $\hat y_i = b_0 + b_1 \cdot 0 = b_0$

    Bei einer Gerade ist $b_0$ also dort, wo $x = 0$. Das ist oft auch
    dort, wo die y-Achse verl√§uft, deswegen nennt man $b_0$ auch
    y-Achsenabschnitt oder *Intercept* auf Englisch.

-   $b_1$ ist das **Regressions*gewicht***.

    Es entspricht der Steigung der Gerade (engl. *Slope*) und ist
    inhaltlich die √Ñnderung im vorhergesagten Wert ($\hat y_i$), wenn
    man $x$ um eine Einheit erh√∂ht.

    Es erinnern sich bestimmt alle an das Dreieck, was man an die Gerade
    zeichnen kann, um die Steigung zu bestimmen.

### Beispiel-Gerade

Hier einmal eine Beispiel-Gerade, beschrieben durch die Gleichung
$\hat y_i = 9 - 1.5x_i$.

-   Intercept $b_0 = 9$
-   Steigung $b_1 = -1.5$

```{r geraden_raten}
.gerade <- function(x, intercept = 0, slope = 1){
  intercept + slope * x
}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(-1, 10), args = list(slope = -1.5, intercept = 9)) +
  geom_segment(aes(x = 2, y = 6, yend = 6, xend = 3), color = "red", 
               arrow = arrow(length = unit(0.03, "npc"))) +
  geom_segment(aes(x = 3, xend = 3, y = 6, yend = 4.5), color = "red",
               arrow = arrow(length = unit(0.03, "npc"))) +
  annotate("text", x = 2.5, y = 6.5, label = "1") +
  annotate("text", x = 3.5, y = 5.125, label = "-1.5") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 10), expand = F) +
  scale_y_continuous(breaks = 0:10) +
  scale_x_continuous(breaks = 0:10) +
  theme_minimal()
```

F√ºr die Steigung berechnest du also die Differenz von y geteilt durch die Differenz von x: $Œîy/Œîx$ = $-1.5/1$.

### Quiz

```{r quizgraph}

ggplot() +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 5, slope = 0.5)) +
  stat_function(fun = `.gerade`, xlim = c(0, 20),
                args = list(intercept = 10, slope = 1), linetype = "dashed") +
  coord_cartesian(ylim = c(0, 20), xlim = c(0, 20)) +
  theme_minimal()
```

```{r guess}
quiz(caption = "Schulmathe auffrischen",
  question_numeric(
  "Welche Steigung (b1) hat die gestrichelte Linie?",
  answer(1, correct = T),
  allow_retry = TRUE
  ),
  question_numeric(
  "Welche Steigung (b1) hat die durchgezogene Linie?",
  answer(0.5, correct = T),
  allow_retry = TRUE
  ),
  question_numeric(
    "Welches Intercept (b0) hat die gestrichelte Linie?",
    answer(10, correct = T),
  allow_retry = TRUE
  ),
  question_numeric(
    "Welches Intercept (b0) hat die durchgezogene Linie?",
    answer(5, correct = T),
  allow_retry = TRUE
  )
  
)
```

Alles klar! Wir sind bereit, $b_0$ und $b_1$ f√ºr unser eigenes Modell zu
bestimmen.

## Fitting the model

Nun wissen wir also, dass das Modell einer einfachen linearen Regression
einfach eine Geradengleichung ist, beschrieben durch die Parameter $b_0$
(Intercept) und $b_1$ (Steigung).

Aber wie genau kommen wir auf diese Parameter? Den Prozess, ein Modell
zu erstellen, was die Realit√§t m√∂glichst gut erkl√§rt, nennt man auf
Englisch *to fit a model*.

$b_0$ und $b_1$ werden so gew√§hlt, dass die Regressionsgerade einen
m√∂glichst kleinen Abstand zu allen Datenpunkten hat.

::: aufgabe
Probiere selbst aus, die Regressionsgerade optimal durch die
Datenpunkte zu legen!
:::

</br>

```{r shiny_ui, echo=FALSE}
sliderInput("b0", "Intercept b0:", min = -1.5, max = 1.5, value = 1, step = 0.0001)
sliderInput("b1", "Slope b1:", min = 0, max = 0.15, value = 0, step = 0.0001)
plotOutput("distPlot")
actionButton("show", "Antwort einreichen & L√∂sung anzeigen", class = "btn-default")
tableOutput("estimate")
plotOutput("optimumPlot")
```

```{r shiny_server, context="server"}
  # Prerequisites: Daten m√ºssen noch mal erstellt werden weil shiny app verwendet eigenes environment
  require(ggplot2)
  # sysinfo <- Sys.info()
  # if(sysinfo["sysname"] == "Linux") require(Cairo) #bessere Grafik unter Linux, ist aber nur h√∂rensagen und nicht n√∂tig

  trees$diameter <- datasets::trees$Girth * 2.54 # Inch in cm
  trees$volume <- datasets::trees$Volume * 0.0283168466 #  ft¬≥ in m¬≥
  fit <- lm(volume ~ diameter, data = trees)

  # Wahre Werte f√ºr b0 und b1 bestimmen
  coefs <-  fit |> coef() |> as.numeric()

  # Minimale QS_res bestimmen
  sum_res_min <- (trees$volume - predict(fit))^2 |> sum()

  # L√∂sungsplot
  optimumPlot <- ggplot(trees, aes(x = diameter, y = volume)) +
    geom_point() +
    geom_abline(intercept = coefs[1], slope = coefs[2], color = "blue") +
    geom_linerange(x = trees$diameter, ymin = trees$volume, ymax = predict(fit),
                   linetype = "solid", colour = "red") +
    annotate(geom = "label", x = 50, y = 0.3, hjust = 0, col = "red", size = 5,
             label = paste0("Summe der \nquadrierten\nResiduen \n=", round(sum_res_min, 4))) +
    theme_minimal() +
    labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)", title = "beste Sch√§tzung") +
    coord_cartesian(xlim = c(20, 55), ylim =c(0, 2.5), expand = F)

  # Reaktive Vorhersage (f√ºr rote Linien)
  pre <- reactive(input$b0 + input$b1 * trees$diameter)

  # reaktive Summe der quadrierten Residuen
  res <- reactive(sum((trees$volume - pre()) ^ 2))

  # Grading Function
  .estimate_grader <- function(x){
    real <- c(coefs, sum_res_min)
    deviation <- (x - real) * 100 / real
    data.frame("Ihre Sch√§tzung" = x,
               "beste Sch√§tzung" = real,
               "Abweichung" = paste(round(deviation, 2), "%"),
               row.names = c("Intercept", "Slope", "summierte quadrierte Residuen"),
               check.names = FALSE)
  }

  # Check Button Logic
  observeEvent(input$show, {
    estimate_cache <- c(input$b0, input$b1, res())
    # updateSliderInput(inputId = "b0", value = coefs[1])
    # updateSliderInput(inputId = "b1", value = coefs[2])
    output$estimate <- renderTable(.estimate_grader(estimate_cache),
                                   rownames = TRUE, digits = 4)
    output$optimumPlot <- renderPlot(optimumPlot)
  })

  # Plot
  output$distPlot <- renderPlot({

  ggplot(trees, aes(x = diameter, y = volume)) +
    geom_point() +
    geom_abline(intercept = input$b0, slope = input$b1, color = "blue") +
    geom_linerange(x = trees$diameter, ymin = trees$volume, ymax = pre(),
                   linetype = "solid", colour = "red") +
    annotate(geom = "label", x = 50, y = 0.3, hjust = 0, col = "red", size = 5,
             label = paste0("Summe der \nquadrierten\nResiduen \n=", round(res(), 5))) +
    theme_minimal() +
    labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)") +
    coord_cartesian(xlim = c(20, 55), ylim =c(0, 2.5), expand = F)

})
```

</br>

Hervorragend! Du hast jetzt hoffentlich ein intuitives Gef√ºhl f√ºr die
Problemstellung bekommen.

Wie du gesehen hast, gab es auch eine durch den Computer berechnete
‚Äûbeste Sch√§tzung‚Äú - vielleicht hast du das einfach so hingenommen,
aber was qualifiziert diese Sch√§tzung eigentlich dazu, sich die ‚Äûbeste‚Äú
zu nennen?

Ein wichtiges Konzept sind dabei die Residuen.

### Residuen

Die oben in rot gekennzeichneten vertikalen Abst√§nde zwischen realen
Werten und den vorhergesagten Werten nennt man auch **Residuen**, also
"√úberbleibsel". Residuen stellen die Abweichung der Realit√§t vom Modell
dar, also den "Fehler", den das Modell nicht erkl√§ren kann.

Offensichtlich ist es daher besser, wenn die Residuen kleiner sind, als
wenn sie sehr gro√ü sind. **Kleine Residuen bedeuten, dass das Modell n√§her an der Realit√§t liegt**.

#### Formel

Ein Residuum $e$ (wie Error) der Messung $i$ errechnet sich aus:
$$e_i = y_i - \hat y_i$$

dabei gilt:

-   $y_i$ = tats√§chlicher Wert

-   $\hat y_i$ = vorhergesagter Wert

#### Methode der kleinsten Quadrate

Eine M√∂glichkeit, zu definieren wann eine Regressionsgerade optimal an
die Daten angepasst ist, zu sagen:

*‚ÄûDie Regressionsgerade liegt optimal, wenn die Summe der quadrierten 
Residuen minimal ist.‚Äú*

$$
\sum_{i = 1}^n \left(e_i\right)^2 = min
$$

Dahinter steht ein grundlegendes Konzept, die ‚Äû**Methode der kleinsten
Quadrate**‚Äú (*least squares*), was eine weit verbreitete Sch√§tzmethode
ist.

Nat√ºrlich hat *least squares estimation* auch Nachteile, zum Beispiel
durch das Quadrieren hohe Empfindlichkeit gegen√ºber Extremwerten.
Deswegen sei hier gesagt: es gibt auch andere Methoden, jedoch ist
*least squares* der Standard.

<details>

<summary><a>‚ñº \* Hintergr√ºnde: analytische L√∂sung der
Minimierung</a></summary>

::: infobox
##### Woher kommen die Formeln f√ºr $b_1$ und $b_0$?

Das tolle an der Methode der kleinsten Quadrate ist, dass es f√ºr die
Regressionskoeffizienten eine analytische L√∂sung gibt, das hei√üt,
mathematische Formeln, mit denen sie *garantiert* so bestimmt werden,
dass die quadrierten Residuen minimal sind.

Das hier ist die Ausgangslage, das Kriterium f√ºr eine optimal liegende
Gerade: Die Summe der quadrierten Residuen soll minimal sein.

$$
\sum_{i = 1}^n \left(e_i\right)^2 = \sum_{i = 1}^n \left( y_i - \hat y_i \right)^2 = min
$$

Da quadrierte Werte immer positiv sind, erreicht man, dass sich negative
und positive Abweichungen beim Summieren nicht aufheben. Au√üerdem stellt
das Quadrieren weitere g√ºnstige mathematische Eigenschaften f√ºr das
Minimieren her.

##### Einsetzen der Regressionsgleichung f√ºr $\hat y$:

$$
\sum_{i = 1}^n \left( y_i - (b_0 + b_1 \cdot x_i) \right)^2 = min
$$

##### Minimieren:

Du hast vermutlich an den Schiebereglern selbst bemerkt - es
funktioniert nicht, nur einen der beiden Koeffizienten isoliert zu
optimieren, sondern du musst verschiedene Kombinationen ausprobieren.

√Ñhnlich ist es bei der Herleitung allgemeiner Formeln. Sie funktioniert
√ºber das Nullsetzen der beiden partiellen Ableitungen nach $b_0$ und
nach $b_1$. Es gibt gute externe Quellen dazu, z. B. im [wikibook √ºber
lineare
Regression](https://de.wikibooks.org/wiki/Statistik:_Regressionsanalyse#Einfaches_lineares_Regressionsmodell).

Daraus leiten sich letzten Endes folgende Formeln ab:

```{=tex}
\begin{align}
b_1 &= \frac{cov(x,y)}{var(x)}
\\
b_0 &= \bar y - b_1 \cdot \bar x
\end{align}
```
Der Key-Takeaway ist: Diese Formeln liefern per mathematischer
Definition die **optimale Regressionsgerade mit minimalen Residuen**!

Amazing.üåà
:::

</details>

</br>

<details>

<summary><a>‚ñº \* Exkurs: iterative L√∂sung der Minimierung</a></summary>

::: infobox
Das Gegenst√ºck zu einer analytischen L√∂sung ist ein **iterativer Ansatz**,
also ‚ÄûTrial & Error‚Äú, wie du es oben selbst vermutlich auch gemacht
hast. Das ist ebenso eine weit verbreitete Methode, die quadrierten
Residuen zu minimieren, besonders dann, wenn die mathematischen
Voraussetzungen f√ºr die klassische L√∂sung nicht gegeben sind (das sind
einige, siehe Kapitel ‚ÄûVoraussetzungen‚Äú). Pr√§destiniert f√ºr die Trial &
Error-L√∂sung sind Machine-Learning-Algorithmen. Wir werden keine dieser
Algorithmen in R verwenden, aber ich m√∂chte darauf hinweisen, dass es
das gibt.

Hier eine Illustration des Fitting-Prozesses, allerdings mit anderen
Daten. Die Grafik zeigt wie ein Machine-Learning-Algorithmus sich
Schritt f√ºr Schritt der optimalen Regressionsgerade ann√§hert, und die
mittleren quadrierten Residuen (Mean Squared Error, MSE) immer kleiner
werden, bis es nicht kleiner geht.

![](images/iterative_fitting.gif){width="80%"}

Source:
[ghbat.com](https://gbhat.com/machine_learning/linear_regression.html)
:::

</details>

</br>

#### √úbung Residuen

::: aufgabe
**1.** Um die Formeln noch mal selbst anzuwenden, ist hier eine kleine
Praxisaufgabe.

In der Tabelle `trees_predict` finden Sie zwei Spalten: Das tats√§chliche
Volumen ($y_i$, `volume`) und das vorhergesagte Volumen ($\hat y_i$, `predict`).

a)  Schauen Sie sich `trees_predict` an.
b)  Berechnen Sie eine neue Spalte mit den Residuen f√ºr jeden einzelnen
    Messwert! (`Residuen <- tats√§chlicher Wert - vorhergesagter Wert`)
c)  Berechnen Sie die Summe der quadrierten Residuen!
    (`Sum(residuen ^ 2)`)
:::

</br>

```{r predict, exercise = TRUE, exercise.setup = "silentsetup"}
trees_predict
```

```{r predict-hint}
# a)
head(trees_predict)

# b)
trees_predict$resid <- trees_predict$XXX - trees_predict$predict

# c)
sum(XXX ^ 2)
```

```{r predict-solution}
# a)
head(trees_predict)

# b)
trees_predict$resid <- trees_predict$volume - trees_predict$predict

# c)
sum(trees_predict$resid ^ 2)
```

```{r predict-question}
question_numeric("Welchen Wert hast du f√ºr die summierten quadrierten Residuen herausbekommen? (3 Nachkommastellen)",
                 answer(0.420, correct = T, message = "Das ist genau der gleiche Wert wie oben bei der Aufgabe mit den Schiebereglern."), tolerance = 0.001, allow_retry = T)
```

</br>

Hang on! In den n√§chsten Kapiteln wird es praktisch: Wie kann ich die Voraussetzungen pr√ºfen? Wie ein Modell in *R* erstellen und mir die **Koeffizienten** ausrechnen lassen? Und die
wichtige Frage wird sein: Wo finde ich die relevanten Ergebnisse, und wie interpretiere ich sie?

![](images/shuffling_tree.gif)

## Vorraussetzungen pr√ºfen

Hier ein kleiner √úberblick, dar√ºber, wie die Voraussetzungen einer Regression
in R gepr√ºft werden k√∂nnen:

+-------------------------------------+--------------------------------+
| Vorannahme                          | √úberpr√ºfung                    |
+=====================================+================================+
| 1.  metrische Variablen             | Vorwissen √ºber die Erhebung    |
+-------------------------------------+--------------------------------+
| 2.  Zuf√§llige Stichprobe            | Vorwissen √ºber die Erhebung    |
+-------------------------------------+--------------------------------+
| 3.  Linearer Zusammenhang in den    | Streudiagramm                  |
|     Daten                           |                                |
+-------------------------------------+--------------------------------+
| 4.  Es gibt Variation beim          | Streudiagramm                  |
|     Pr√§diktor                       |                                |
+-------------------------------------+--------------------------------+


: Vor dem Modellieren

+----------------------------------+-----------------------------------+
| Vorannahme                       | √úberpr√ºfung                       |
+==================================+===================================+
| 5.  Homoskedastizit√§t            | `plot(fit)`                       |
+----------------------------------+-----------------------------------+
| 6.  Unkorreliertheit der         | `plot(fit)`                       |
|     Residuen mit der UV          |                                   |
+----------------------------------+-----------------------------------+
| 7.  keine Autokorrelation der    | `car::durbinWatsonTest(fit)`      |
|     Residuen                     |                                   |
+----------------------------------+-----------------------------------+
| 8.  Normalverteilung der         | Q-Q-Plot: der zweite Plot bei     |
|     Residuen                     | `plot(fit)`                       |
|                                  |                                   |
|                                  | Histogramm der                    |
|                                  | standardisierten Residuen         |
|                                  |                                   |
|                                  |                                   |
|                                  | `resid(fit) |> scale() |> hist()` |    
|                                  |                                   |
+----------------------------------+-----------------------------------+

: Nach dem Modellieren

Wir k√∂nnen einige unserer Annahmen also erst nach der Berechnung des Modells 
√ºberpr√ºfen. Das ist f√ºr uns neu! Hier stellen wir dir zun√§chst alle Voraussetzungen
und Tests vor und sp√§ter im √úbungskapitel wenden wir diese auf unseren 
heutiges Beispiel an.

### 1. Metrische Variablen

Sowohl der Pr√§diktor als auch das Kriterium m√ºssen metrisch sein.

<details>

<summary><a>‚ñº \* Hier sind einige andere Verfahren, falls das nicht
gegeben ist: </a></summary>

::: infobox
kategorialer Pr√§diktor + metrisches Kriterium

-   bei zwei Kategorien / Gruppen: $t$-Test

-   bei mehr als zwei Kategorien / Gruppen: Varianzanalyse (ANOVA) oder
    generalisiertes lineares Modell (GLM) mit Dummy-Kodierung

metrischer Pr√§diktor + kategoriales Kriterium:

-   logistische Regression
:::

</details>

</br>

### 2. Zuf√§llige Stichprobe

Im Beispiel mit der sp√§tbl√ºhenden Traubenkirsche w√§re es zum Beispiel
wichtig, dass nicht selektiv nur B√§ume vermessen wurden, die eine
besonders gute Wuchsform hatten, sondern eine ausreichend gro√üe,
zuf√§llige Auswahl aus allen m√∂glichen B√§umen getroffen wurde, damit eine
Normalverteilung angenommen werden kann. Dabei hei√üt ausreichend gro√üe
Stichprobe als Faustregel ungef√§hr $n > 30$.

### 3. Linearer Zusammenhang in den Daten

Manche Daten werden besser nicht durch Geraden erkl√§rt, sondern haben
vielleicht quadratische Zusammenh√§nge, wie in diesem Beispiel:

```{r}
data <- data.frame(temperatur=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60),
                   enzymaktivit√§t=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))


straight_fit <- lm(enzymaktivit√§t ~ temperatur, data = data)
data$temperatur2 <- data$temperatur^2
quad_fit <- lm(enzymaktivit√§t ~ temperatur + temperatur2, data = data)
zeit <- seq(0, 60, 0.1)
prediction <- predict(quad_fit, list(temperatur = zeit, temperatur2 = zeit^2))

plot(enzymaktivit√§t ~ temperatur, data = data)
abline(straight_fit, col = "red")
lines(zeit, prediction, col = "blue")
```

Die Gerade erkl√§rt die Daten nicht gut und trifft falsche Vorhersagen,
w√§hrend die quadratische Funktion sehr nah an der Realit√§t liegt.
(Fiktive Daten)

Die √úberpr√ºfung einer linearen Beziehung in den Daten erfolgt visuell
√ºber ein Punktdiagramm. Die Punkte sollten sich alle durch eine Gerade
beschreiben lassen und keine *systematischen* Abweichungen von der
Linearit√§t haben. Zuf√§llige Streuung hingegen ist komplett in Ordnung.

Beispiele:

```{r plotmaking, fig.height=5, fig.width=6}
p1 <- ggplot(rtutorials::concrete, aes(x = coarse_aggregate, y = compressive_strength)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "1")

p2 <- ggplot(rtutorials::heated, aes(x = edvisits, y = maxtemp)) +
  geom_point(alpha = 0.18) +
  theme_void() +
  labs(title = "2")

p3 <- ggplot(rtutorials::airfoil, aes(x = angle, y = displacement_thickness)) +
  geom_point(alpha = 0.03) +
  theme_void() +
  labs(title = "3")

p4 <- ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(alpha = 0.2) +
  theme_void() +
  labs(title = "4")

p5 <- ggplot(women, aes(x = height, y = weight)) +
  geom_point(alpha = 0.5) +
  theme_void() +
  labs(title = "5")
  
p6 <- ggplot(faithful, aes(x = eruptions, y = waiting)) +
  geom_point(alpha = 0.34) +
  theme_void() +
  labs(title = "6")

gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6)
```

```{r plotquiz}
question_checkbox("Bei welchen dieser Plots w√ºrden Sie eine zugrunde liegende lineare Beziehung vermuten?",
                  answer("1", message = "(1) Hier ist es schwer zu sagen - die Daten enthalten sehr viel Streuung. Auf√§llig ist die Konzentration viele Punkte auf bestimmten x-Werten."),
                  answer("2", message = "(2) Diese Beziehung zwischen Notaufnahmebesuchen (y) und Hitze (x) folgt eher einer Kurve als einer Gerade"),
                  answer("3", message = "(3) Hier liegt scheinbar eher eine Exponentialfunktion zu Grunde, keine Gerade"),
                  answer("4", correct = T, message = "(4) Hier k√∂nnte eine Gerade die Punkte gut beschreiben."),
                  answer("5", correct = T, message = "(5) Es handelt sich um den Zusammenhang zwischen K√∂rpergr√∂√üe und Gewicht."),
                  answer("6", correct = T, message = "(6) Auch hier l√§sst sich eine lineare Beziehung vermuten."),
                  allow_retry = TRUE
         )
```

### 4. Variation des Pr√§diktors

*Wie sieht ein Plot ohne Variation des Pr√§diktors aus?*

Alle Punkte liegen auf dem selben $x$-Wert! In diesem Fall hilft $x$
√ºberhaupt nicht, Varianz in $y$ zu erkl√§ren. Die beste Vorhersage f√ºr
$y$ ist dann der Mittelwert $\bar y$.

Dieser Fall ist in der Praxis extrem unwahrscheinlich, da es meistens
ein bisschen zuf√§llige Variation gibt.

```{r novariation, fig.height=5, fig.width=5}
plot(x = rep(20, 100), y = rnorm(100), yaxt = "n", xaxt = "n", xlab = "x", ylab = "y")
```

Diese Betrachtungen kann man ohne Modell bereits vor der Analyse durchf√ºhren. 

F√ºr die letzten Voraussetzungen (5-8) brauchen wir unser bestehendes Modell.

### 5. Homoskedastizit√§t

Wieder mal ein langes und kompliziertes Wort, das dir da begegnet. Aber
nicht verzagen, es meint lediglich: **die Varianz der Daten sind √ºber
die Variable gleich verteilt**. Das bedeutet, dass die Streuung der
Residuen um die Regressionslinie f√ºr alle Werte der unabh√§ngigen
Variablen gleich bleibt. Wenn diese Bedingung erf√ºllt ist, spricht man
von Homoskedastizit√§t. Ist sie nicht erf√ºllt, also wenn die Varianz der
Residuen mit den Werten der unabh√§ngigen Variablen variiert, spricht man
von Heteroskedastizit√§t.

Die Einhaltung der Homoskedastizit√§t ist wichtig, da viele statistische
Tests, insbesondere in der linearen Regression, auf der Annahme
basieren, dass diese Bedingung gegeben ist. Heteroskedastizit√§t kann zu
ungenauen Sch√§tzungen und zu irref√ºhrenden Testergebnissen f√ºhren.

Was ist hier gemeint? Schau dir die Grafiken an und versuche zu
verstehen, welche der Variablen keine gleichverteilten Residuen hat.

```{r homoskedasis_grafiken, message=FALSE, warning=FALSE}
# H1 Homosked.
set.seed(1)
x <- 1:1000
y <- rnorm(1000, sd = 100)
df1 <- data.frame(x, y)
h1 <- ggplot(df1, aes(x, y)) +
  geom_point(alpha = 0.34) +
#  geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "1")

y <- 2 + 0.5 * x + rnorm(1000, sd = x)
df2 <- data.frame(x, y)
h2 <- ggplot(df2, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "2")

sd3 <- sin(seq(0, 5, length.out = 1000))^2
y <- rnorm(1000, sd = sd3)
df3 <- data.frame(x, y)
h3 <- ggplot(df3, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "3")

y <- sin(seq(0, 10, length.out = 1000)) + rnorm(1000, sd = 0.5)
df4 <- data.frame(x, y)
h4 <- ggplot(df4, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "4")

y <- 2 + 0.5 * x + rnorm(1000, sd = 30)
df5 <- data.frame(x, y)
h5 <- ggplot(df5, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "5")

y <- 10 - 0.5 * x + rnorm(1000, sd = 1000:1)
df6 <- data.frame(x, y)
h6 <- ggplot(df6, aes(x, y)) +
  geom_point(alpha = 0.34) +
  # geom_smooth(method = "lm", se = F) +
  theme_void() +
  labs(title = "6")

gridExtra::grid.arrange(h1, h2, h3, h4, h5, h6)
```

```{r homoskedasis_question}
question_checkbox("Welche der Grafiken zeigen eine homoskedastische Verteilung?",
                  answer(1, correct = T, message = "(1) Hier ist es offensichtlich"),
                  answer(2, message = "(2) Das klassische Beispiel f√ºr Heteroskedastizit√§t - die Trompetenform"),
                  answer(3), message = "(3) k√∂nnte ein Beispiel f√ºr saisonale Schwankungen sein - heteroskedastisch",
                  answer(4, correct = T, message = "(4) ist eigentlich homoskedastisch, weil die Streuung √ºberall gleich ist, aber es liegt eine Sinuskurve anstelle einer Gerade zugrunde. (Dadurch ist dann auf jeden Fall eine andere Voraussetzung verletzt, n√§mlich die Linearit√§tsannahme)"),
                  answer(5, correct = T, message = "(5) hat eine in der Praxis fast unrealistisch kleine Streuung, aber sie ist √ºberall gleich breit - homoskedastisch"),
                  answer(6, message = "(6) Das klassische Beispiel f√ºr Heteroskedastizit√§t - die Trompetenform"), 
                  allow_retry = T)
```

### 6. Unkorreliertheit der Residuen mit Kriterium

Werden die Residuen zum Beispiel gr√∂√üer, je gr√∂√üer das Kriterium wird,
dann ist das ein Hinweis auf Heteroskedastizit√§t. Vergleichen wir daf√ºr
noch einmal zwei Plots:

```{r, echo=T, message=F, warning=F}
h1 + geom_smooth(method = "lm", se = F)
fit1 <- lm(y ~ x, data = df1)
plot(fit1, which = 1)

h6 + geom_smooth(method = "lm", se = F)
fit6 <- lm(y ~ x, data = df6)

plot(fit6, which = 1)
```

### 7. keine Auto-Korrelation der Residuen

Was bedeutet Auto-Korrelation?

Die altgriechische Vorsilbe ‚ÄûAuto-‚Äú bedeutet ‚ÄûSelbst-‚Äú oder ‚ÄûEigen-‚Äú. Es
geht also um Korrelation der Werte mit sich selbst.

Dabei wissen wir ja eigentlich, dass die Korrelation jeder Variable mit
sich selbst perfekt ist, also 1 betr√§gt. Darum geht es also nicht.

Die Frage hier ist, ob die H√∂he eines Residuums die H√∂he der
benachbarten Residuen beeinflusst. Wenn das √ºberm√§√üig der Fall ist, sind
die Residuen nicht zuf√§llig.

Warum ist das wichtig? Das Modell baut darauf, dass ein linearer
Zusammenhang zugrunde liegt, und dass jegliche Streuung in den Daten um
diese Gerade rein zuf√§lliger Natur ist, also keine Systematik dahinter
steht.

#### √úberpr√ºfung in R

Die Auto-Korrelation kann in R berechnet und ausgegeben werden mitsamt
eines Hypothesentests, der die Nullhypothese $\rho = 0$ testet (‚ÄûDie
Auto-Korrelation betr√§gt in Wahrheit 0‚Äú).

```{r autocor-question}
question_checkbox("Wie m√ºsste der p-Wert aussehen, wenn unsere Daten vermutlich aus einer Population ohne Autokorrelation stammen?",
                  answer("$p \\le 0.05$", message = "Wenn p kleiner als 0.05 w√§re, m√ºssten wir die Nullhypothese verwerfen. Da die Nullhypothese aber enth√§lt, dass die Autokorrelation in Wahrheit 0 ist, w√§re das also nicht der erw√ºnschte Fall, diese Annahme zu verwerfen - das w√§re dann ein Hinweis darauf, dass es Autokorrelation gibt."),
                  answer("$p > 0.05$", correct = T, message = "Genau! In diesem Fall halten wir danach Ausschau, dass der Test nicht signifikant wird, der p-Wert gr√∂√üer als 0.05 bleibt, damit die Nullhypothese beibehalten werden darf, da das das ‚Äûerw√ºnschte‚Äú Ergebnis ist."),
                  allow_retry = T)
```

Gepr√ºft wird das mit einem Durbin-Watson-Test. Das Paket `car` stellt
eine Funktion daf√ºr bereit:

Testen wir das zun√§chst an einem extremen Beispiel, was ich simuliert
habe und definitiv Auto-Korrelation enthalten sollte: Plot 4 aus dem
Homoskedastizit√§t-Abschnitt, die Residuen variieren systematisch!

Wenn Auto-Korrelation besteht, dann variieren benachbarte Residuen
gemeinsam.

```{r , message=FALSE, warning=FALSE}
fit4 <- lm(y ~ x, data = df4)
pre4 <- predict(fit4)
h4 + geom_smooth(method = "lm", se = F) +
  geom_linerange(aes(ymin = pre4, ymax = y), color = "red", alpha = .4)

```

```{r, echo = TRUE}
car::durbinWatsonTest(fit4)
```

```{r autocor_question}
question_radio("Was ist deine Hypothesenentscheidung, wenn du den $p$-Wert des Tests mit $\alpha$ = 0.05 vergleichst?",
               answer("H0 beibehalten", message = "Da $p$ < $alpha$ ist der Test signifikant. Bei Signifikanz muss die H0 abgelehnt werden."),
               answer("H0 ablehnen", correct = T,
                      message = "Da $p$ < $\alpha$, ist der Test signifikant."),
               allow_retry = T,
               correct = random_praise("de"),
               incorrect = random_encouragement("de"))
```


### 8. Normalverteilung der Residuen

Eine letzte und zentrale Annahme ist, dass die Residuen normalverteilt
sein m√ºssen.

Eine M√∂glichkeit, das grob visuell zu √ºberpr√ºfen, ist ein Histogramm der 
standardisierten Residuen zu erstellen.

Daf√ºr berechnen wir die Residuen des Models *fit* `resid(fit)` geben dies in eine
Funktion zur Standardisierung `scale()` und lassen uns daraus ein Histogramm erstellen
`hist()`.

```r
resid(fit) |> scale() |> hist()
```

Wir erinnern uns: Ein Histogramm erm√∂glicht, die Verteilung einer
metrischen Variable zu visualisieren.

```{r residuenhist, exercise = T}
resid(fit) |> scale() |> hist()
```

#### visuelle √úberpr√ºfung

Es mag willk√ºrlich erscheinen, rein aus dem Visuellen grob zu bestimmen,
ob etwas normalverteilt ist oder nicht. Mit diesem kleinen Spiel k√∂nnen
Sie Ihre Intuition etwas sch√§rfen:

```{r ui_distribution-guesser}
verteilungen <- c("Normal", "Uniform", "Bimodal")

fluidPage(

  shinyFeedback::useShinyFeedback(),
  shinyjs::useShinyjs(),

  # Application title
  titlePanel("Verteilungen erraten"),


  sidebarLayout(
    sidebarPanel(
      sliderInput("n", "Stichprobengr√∂√üe (aka Schwierigkeitsgrad)",
                  value = 50, min = 30, max = 300, step = 1),
      # sliderInput("bins", "Anzahl Bins", min = 3, value = 20, max = 80, step = 1),
    ),

    mainPanel(
      plotOutput("plot"),
      radioButtons("radio",
                   "Aus welcher Verteilung wurde die Stichprobe gezogen?",
                   verteilungen, selected = character(0)),
      actionButton("button", "N√§chster Plot"),
      tableOutput("table"),
      textOutput("text"),
      actionButton("reset", "Neustart")
    )
  )
)

```

```{r server_distribution-guesser, context = "server"}
# Die App wird separat entwickelt, da sonst f√ºr jeden Test das Laden des gesamten tutorials n√∂tig ist
# source of truth: https://github.com/luk-brue/normal-distribution-guesser
library(shinyFeedback)
library(shinyjs)

verteilungen <- c("Normal", "Uniform", "Bimodal")

  sampler <- function() sample(verteilungen, size = 1)
  zufall <- reactiveValues(sample = sampler())
  counter <- reactiveValues(richtig = 0, falsch = 0, round = 0)

  observeEvent(input$button, counter$round <- counter$round + 1)

  observeEvent(input$n, {
    feedback(inputId = "n", text = "unm√∂glich", color = "darkred",
             show = {input$n < 30})
    feedback(inputId = "n", text = "schwer", color = "red",
             show = {input$n > 30 && input$n <= 39})
    feedback(inputId = "n", text = "mittelschwer", color = "orange",
             show = {input$n > 39 && input$n <= 50})
    feedback(inputId = "n", text = "medium", color = "yellow",
             show = {input$n > 50 && input$n <= 100})
    feedback(inputId = "n", text = "leicht", color = "darkgreen",
             show = {input$n > 100 && input$n <= 200})
    feedback(inputId = "n", text = "sehr leicht", color = "lightgreen",
             show = {input$n > 200})

  })

  observeEvent(input$reset, {
    zufall$sample <- sampler()
    counter$richtig <- 0
    counter$falsch <- 0
    counter$round <- 0
  })

  output$text <- renderText({

    paste("Erfolgsquote:",
          if(counter$round >= 5){
            paste(
              round(
                counter$richtig / (counter$richtig + counter$falsch) * 100,
                digits = 2
              ),
              "%")
          } else {
            "wird erst ab 5 Versuchen angezeigt"
          }
    )
  })

  output$plot <- renderPlot({
    input$button
    x <- switch(zufall$sample,
                "Normal" = rnorm(input$n),
                "Uniform" = runif(input$n),
                "Bimodal" = c(rnorm(input$n / 2), (rnorm(input$n / 2) + 5))
    )
    df <- data.frame(x)
    bins <- as.integer(nclass.Sturges(x) * 1.4)
    ggplot(df, aes(x = x)) +
      geom_histogram(bins = bins, fill = "gray", color = "black") +
      theme_minimal()
  })
  observeEvent(input$radio,
               {
                 req(input$radio)
                 test <- input$radio == zufall$sample
                 if(test){
                   counter$richtig <- counter$richtig + 1
                   showToast(type = "success",
                             message = "",
                             keepVisible = TRUE)
                 } else {
                   counter$falsch <- counter$falsch + 1
                   showToast(type = "error", message = zufall$sample,
                             keepVisible = TRUE)
                 }
                 disable("radio")
               })
  observeEvent(input$button,
               {updateRadioButtons(session, inputId = "radio",
                                   selected = character(0))
                 zufall$sample <- sampler() # Zufallsgenerator
                 enable("radio")
                 hideToast(animate = F)
               })

  output$table <- renderTable({
    data.frame(richtig = counter$richtig, falsch = counter$falsch)
  }, digits = 0)
```



## Regression berechnen

In R erh√§lt man die Regressionskoeffizienten $b_0$ und $b_1$ √ºber die
Funktion `lm()` (*linear model*).

Fokussieren wir uns zun√§chst auf die Eingabe, den Output schauen wir uns
im n√§chsten Schritt an.

### Funktion

``` r
# Modell erstellen und abspeichern 
fit <- lm(outcome ~ predictor, data = datensatz)

# Modell ansehen
summary(fit)
```

### Code Breakdown

`lm()`:

-   `outcome ~ predictor`: ist eine Formel, die R sagt: **explain `outcome` by
    `predictor`**.

    Ich finde es hilfreich, die Tilde (~) im Kopf zu lesen als: *explained
    by* oder auf deutsch "erkl√§rt durch". Links der Tilde kommt immer
    die Variable hin, die wir erkl√§ren oder vorhersagen wollen, und
    rechts der Tilde die Pr√§diktoren.

-   `data = datensatz`: Da wir die die Variablennamen ohne
    `data$...` davor verwenden wollen, sagen wir *R*  im Argument `data`, wo die
    Variablen zu finden sind.

`fit <-`:

-   Schlie√ülich speichern wir das Modell in einem Objekt, was wir `fit`
    genannt haben (frei ausgew√§hlter Name).

`summary(fit)`:

-   ruft eine Zusammenfassung unseres gespeicherten Modells auf, was uns
    zur Ausgabe f√ºhrt.

### Ausgabe

Der Output sieht eventuell √ºberw√§ltigend aus, weil er eine ziemlich hohe
Informationsdichte hat. Das macht aber gar nichts, denn wir richten
unsere Aufmerksamkeit zun√§chst gezielt auf den Abschnitt "Coefficients:"

```         
Call:
lm(formula = volume ~ diameter, data = trees)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.228386 -0.087972  0.004303  0.098961  0.271468 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
diameter     0.056476   0.002758   20.48  < 2e-16 ***
---
Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1

Residual standard error: 0.1204 on 29 degrees of freedom
Multiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```
Dort finden wir die Parameter $b_0$ (Intercept) und $b_1$ (Steigung). Und
zwar in der Spalte "Estimate".

```         
Coefficients:
             Estimate    
(Intercept) -1.046122   <--- b0
diameter     0.056476   <--- b1
```

Daraus k√∂nnen wir entnehmen:

-   Intercept $b_0 = -1.0461$
-   Steigung $b_1 = 0.0565$

<details>

<summary><a>‚ñº \* Exkurs: Welchen Algorithmus verwendet
`lm()`?</a></summary>

::: infobox
Lustigerweise wird unter der Haube von `lm()` letztlich nach vielen
Zwischenschritten eine uralte Funktion in einer anderen
Programmiersprache, n√§mlich FORTRAN, aufgerufen, um die Schwerarbeit zu
machen. Sie stammt aus dem `LINPACK`-Paket, was original f√ºr
Supercomputer in den 70ern und 80ern verfasst wurde, um lineare
Gleichungssysteme zu l√∂sen. Es ist scheinbar trotz des Alters nach wie
vor effizient darin, *least squares optimization* zu betreiben.

Hier gibt es einen interessanten Blogbeitrag [(A deep dive into how R
fits a linear
model)](https://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html)
dazu, der jeden einzelnen Schritt auflistet bis zur untersten Ebene, dem
FORTRAN-Herzst√ºck von `lm()`.

Der verwendete Algorithmus basiert auf QR-Zerlegung, und ist durch die
Methode der ‚ÄûHouseholder-Transformationen‚Äú implementiert. Ich wei√ü
nicht, was das ist - aber
[Wikipedia](https://de.wikipedia.org/wiki/QR-Zerlegung) wei√ü es :)
:::

</details>

</br>

Jetzt bist du dran!

::: aufgabe
Erstelle mit `lm()` ein lineares Modell zur Erkl√§rung des Volumens (`volume`)
durch den Durchmesser (`diameter`) im `trees`-Datensatz und speichere es ab als
`fit`.

Lasse dir mit `summary()` eine Zusammenfassung des gespeicherten
Modells ausgeben! 

Wenn du m√∂chtest, kannst du auch hier die `pander::pander()` Funktion nutzen, 
um einen besser lesbaren Output zu erhalten.

Finde $b_0$ und $b_1$!
:::

```{r model, exercise = TRUE, exercise.setup = "silentsetup"}

```

```{r model-hint}
# passe den Code an:
modell <- lm(outcome ~ predictor, data = dataframe)

summary(modell)

```

```{r model-solution}
fit <- lm(volume ~ diameter, data = trees)

summary(fit) |> pander()

```

Die Berechnung und das Ablesen scheint zu klappen, dann geht es weiter zur Interpretation.

</br>

![](./images/menschbaum.JPG){width="40%"}

[Bild: Wikimedia Commons, CC 0]

## Interpretation der Koeffizienten

Jetzt haben wir herausgefunden, dass $b_0 = -1.0461$ und $b_1 = 0.0565$ - aber
was bedeuten diese Werte inhaltlich?

Zun√§chst einmal k√∂nnen wir uns merken, dass die Regressionskoeffizienten
immer **in Einheiten der vorhergesagten Variable** interpretiert werden. In unserem 
Fall also als Volumen (m¬≥).

### $b_0$ (Intercept)

Generell gilt:
*‚Äû$b_0$ ist der vorhergesagte Wert, wenn der Pr√§diktor den Wert 0
annimt."*

Bezogen auf unser Beispiel:
**‚ÄûWenn der Durchmesser eines Baumes 0 cm betragen w√ºrde, w√§re das
vorhergesagte Holzvolumen -1.05 Kubikmeter."**

Das ergibt inhaltlich wenig Sinn - es gibt keinen Baum, der er einen 
Durchmesser von 0 cm hat, und es gibt auch kein negatives Volumen. Das ist mit 
allen Daten so, bei denen $x = 0$ nicht im sinnvollen Wertebereich ist.
Man *kann* dann f√ºr eine bessere Interpretierbarkeit die Daten zentrieren,
so dass $x = 0$ einen sinnvollen Wert darstellt. Siehe den Exkurs: "Zentrieren".

### $b_1$ (Steigung)

Generell:
*‚Äû$b_1$ ist die vorhergesagte √Ñnderung, wenn der Pr√§diktor um **eine Einheit** 
erh√∂ht wird."*

F√ºr unser Beispiel:
*‚ÄûPro Zentimeter Durchmesser mehr steigt unser erwartetes Volumen um
0.0565 Kubikmeter."*

::: gelb
Das Regressionsgewicht $b_1$ ist eng verwandt mit der Korrelation, aber
es gibt einen wichtigen Unterschied: Es ist nicht ‚Äûsymmetrisch‚Äú wie eine
Korrelation. Es l√§sst immer nur **R√ºckschl√ºsse in eine Richtung** zu, z.B.
vom Durchmesser auf das Volumen, nicht umgekehrt. Das ist das Spezielle
an der linearen Regression!

<details>

<summary><a>‚ñº F√ºr diejenigen, die das anhand von Formeln verstehen
m√∂chten:</a></summary>

Formel f√ºr die Korrelation

$$
r = \frac{cov(x, y)}{s_x \cdot s_y}
$$ 

Formel f√ºr das Regressionsgewicht:

$$
b_1 = \frac{cov(x, y)}{s_x^2}
$$ Hier fehlt im Nenner die Varianz von y

</details>

:::

</br>

![](images/massband.jpg){width="40%"}


## Modellg√ºte $R^2$

*Was ist $R^2$?*

$R^2$ definiert, **wie gut ein Modell an die Daten angepasst** ist und hei√üt
deswegen auch ‚Äû**Anpassungsg√ºte**‚Äú.

Um $R^2$ zu verstehen, ist es hilfreich zu verstehen, dass man die
**Varianz** im Kriterium (Schwankungen im Holzvolumen) in
zwei Teile **aufteilen** kann:

-   in einen durch das Modell (0.056 \* Durchmesser - 1.04) **erkl√§rten
    Teil**
-   und in den **Fehler**, der **nicht erkl√§rbare** zuf√§llige **Abweichung** vom
    Modell ist.

Aus beiden Teilen zusammen ergibt sich die Gesamtvarianz des Kriteriums.

Die Formel f√ºr $R^2$ ist dann:

$$
R^2 = \frac{\text{durch das Modell erkl√§rte Varianz in y}}{\text{Gesamte Varianz in y}} = \frac{\sum_{i=1}^n( \hat y_i - \bar y)^2}{\sum_{i=1}^n(y_i - \bar y)^2}
$$

### Im Output finden

Konzentrieren wir uns also auf einen neuen Bereich im *R*-Output:

```{r modelgoodness, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(volume ~ diameter_centered, data = trees)
summary(fit_centered)
```

Das $R^2$ (auch Determinationskoeffizient) finden wir in der vorletzten Zeile, bei:

```         
Multiple R-squared:  0.9353   <------- R¬≤
```

Also ist $R^2 = 0.9353$.

<details>

<summary><a>‚ñº Warum hei√üt es im Output ‚ÄûMultiple
R-squared‚Äú?</a></summary>

::: infobox
Der Pr√§fix "Multiple" ist wie eine Warnung: Wenn mehr als ein Pr√§diktor
im Modell verwendet werden, wird $R^2$ mit jedem Pr√§diktor zwangsl√§ufig
h√∂her, au√üer man korrigiert diese Verzerrung. Das hei√üt dann
`Adjusted R-squared` und das ignorieren wir hier alles, weil wir nur
einfache lineare Regression machen, also nur einen Pr√§diktor verwenden.
:::

</details>

#### Interpretation

Wir k√∂nnen mit $R^2$ also ausdr√ºcken, wie gut unser Modell unsere Daten beschreibt:

Generell w√§re die Interpretation wie folgt:
‚Äû$R^2 \cdot 100 =$ % der Varianz in der *abh√§ngigen Variable* k√∂nnen durch
*das Modell* aufgekl√§rt werden.‚Äú

F√ºr unser Beispiel also:
*‚Äû93,53% der Varianz **im Holzvolumen** k√∂nnen durch das Modell aufgekl√§rt
werden.‚Äú*

Das Modell besteht in diesem Fall nur aus einem Pr√§diktor, deswegen
k√∂nnen wir auch ‚ÄûModell‚Äú durch den Pr√§diktor ersetzen:

‚Äû93,53 % der Varianz im Holzvolumen k√∂nnen auf den **Durchmesser** 
zur√ºckgef√ºhrt werden.‚Äú

::: gelb
Die H√∂he des Wertes ist nicht wirklich gut vergleichbar, da es vom
Forschungsgebiet abh√§ngt, wie viel Varianzaufkl√§rung als "gut" gilt.
√úber 90% Varianzaufkl√§rung durch das Modell sind aber fast schon
unglaublich und das kommt in der Praxis nur selten vor.
:::


<details>

<summary><a>‚ñº Ex: Zusammenhang mit Korrelationskoeffizient $r$ </a></summary>

Im Fall der einfachen linearen Regression, aber nicht bei multipler
Regression, entspricht $R^2$ der quadrierten Pearson-Korrelation $r$ des
Pr√§diktors mit dem Kriterium. Daraus folgt: $\sqrt{R^2} = r$

#### √úbung

::: aufgabe
Berechnen Sie den Korrelationskoeffizienten auf zwei Wegen:

-   als Wurzel von $R^2$ mit der Funktion `sqrt(x)` - square root of x
-   mit der `cor(variable1, variable2)`-Funktion

Sind die Ergebnisse identisch?
:::

```{r cor, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Korrelation"}
r2 <- 0.9353

```

```{r cor-hint}
r2 <- 0.9353
# f√ºge den fehlenden Code ein:
sqrt(X)
cor(trees$Y, trees$Z) 

```

```{r cor-solution}
r2 <- 0.9353
sqrt(r2)
cor(trees$diameter, trees$volume) 
# 0.967109 == 0.967112
# Die kleine Abweichung ist ok, wir runden auf 4 Nachkommastellen, weil R¬≤ uns nicht pr√§ziser gegeben ist.
```

```{r korrelationsfrage}
question_numeric("Geben Sie Ihren errechneten Wert der Pearson-Korrelation zwischen Volumen und Durchmesser ein, gerundet auf 4 Nachkommastellen!",
                 answer(0.9671, 
                        correct = TRUE, 
                        message = "Wir runden auf 4 Nachkommastellen, weil R¬≤ uns nicht pr√§ziser gegeben ist."),
                 allow_retry = TRUE)
```

#### Reminder: Interpretation Korrelation

Wie interpretiert man noch mal den Korrelationskoeffizienten? Siehe
Tutorial "Korrelation".

```{r korrelationswiederholung}
question_checkbox("Wie interpretiert man noch mal den Korrelationskoeffizienten?",
                 answer("Als St√§rke des Zusammenhangs", 
                        correct = TRUE, 
                        message = "Richtig. Die gr√∂√üe von |*r*| sagt uns die St√§rke des Zusammenhangs. Alles ab ca. |.5| ist nach Cohen (1988) ein starker Zusammenhang."),
                 answer("Als Richtung des Zusammenhangs", 
                        correct = TRUE, 
                        message = "Die Vorzeichen sagen uns die Richtung des Zusammenhangs."),
                 answer("Als Richtung des Effektes", 
                        message = "Wir runden auf 4 Nachkommastellen, weil R¬≤ uns nicht pr√§ziser gegeben ist."),
                 allow_retry = TRUE)
```

Demnach ist unser $\sqrt{R^2} = r$ mit 0.9671 ein starker positiver Zusammenhang‚Äú.

</details>

Nun hast du dich ausgiebig mit der Modellg√ºte befasst. Allerdings
k√∂nnen wir damit noch keine allgemeinen Aussagen treffen √ºber
Kausalit√§t, denn $R^2$ ist ein rein deskriptives, also *beschreibendes*
Ma√ü unserer Modellg√ºte. Um Schlussfolgerungen abzuleiten, m√ºssen wir wieder 
Inferenzstatistik betreiben (schlie√üende Statistik).

## Signifikanztests

Alles, was wir bisher beobachtet haben, gilt zwar in unserer kleinen
Stichprobe von 31 sp√§tbl√ºhenden Traubenkirschen, aber wir wissen nicht,
ob wir die Ergebnisse auch verallgemeinern k√∂nnen.

F√ºr die multiple Regression wird in 2 Arten von Signifikanztests unterschieden:
- Test, ob das Modell insgesamt einen Erkl√§rungsbeitrag leistet (**Gesamtmodell**)
- Test, ob die Regressionskoeffizienten einen Erkl√§rungsbeitrag leisten (**Lokalmodell**)

Da in der einfachen Regression (mit einem Pr√§diktor) das Gesamtmodell dem 
Lokalmodell gleicht, brauchen wir keine zus√§tzlichen Lokalmodelle testen, um den 
alleinigen Einfluss der jeweiligen anderen Pr√§diktoren zu testen (wie es bei der
multiplen Regression der Fall w√§re). Du solltest jedoch nur Lokaltests rechnen, 
wenn dein Gesamtmodell signifikant ist! Wie du das √ºberpr√ºfst findest du als 
n√§chstes heraus:

### Test des Gesamtmodells

Mit dem Test des Gesamtmodells wollen wir herausfinden, ob der Einbezug des Pr√§diktors (UV) die Vorhersage des Kriteriums (AV) signifikant gegen√ºber der simplen Vorhersage √ºber
den Mittelwert des Kriteriums (AV) verbessert.

Der Hintergrundgedanke ist folgender:

Wir k√∂nnten eine Stichprobe gezogen haben, in welcher der
Durchmesser und das Holzvolumen **zuf√§llig** zusammenh√§ngen, obwohl das
eigentlich generell im ganzen Wald nicht der Fall ist.

![](images/ftest.jpg){width="60%"}

Die Nullhypothese $H_0$ des Gesamtmodells lautet daher: 

$H_0 = R^2$ betr√§gt in Wahrheit 0.
$H_1 = R^2$ ist ungleich 0.

### R-Output

Wir finden den Test des Gesamtmodells in der allerletzten Zeile des
R-Outputs:

```{r ftest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(volume ~ diameter_centered, data = trees)
summary(fit_centered)
```

Der Test des Gesamtmodells ist ein $F$-Test, da zwei Varianzen ins
Verh√§ltnis gesetzt werden: Die durch das Modell erkl√§rte Varianz, und
die nicht erkl√§rte Varianz. (Immer, wenn Varianzen verglichen werden,
klingt das in deinen Ohren nach $F$-Test. Zumindest sollte das so sein.)

Wie genau die `F-statistic` zustande kommt kann hier nicht ausf√ºhrlich behandelt
werden. Das sind nur Zwischenschritte auf dem Weg zur Berechnung des
$p$-Werts. **Wichtig ist, dass du den $p$-Wert als Endresultat
interpretieren kannst, egal ob er von einem $F$-, $t$- oder $z$-Test
ist.**

Hier ist die relevante Zeile:

```         
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16   <--- p-Wert Gesamtmodelltest
```

```{r pgesamtquestion}
learnr::question_radio("Wie w√ºrde darauf basierend deine Hypothesenentscheidung f√ºr das Gesamtmodell aussehen? (Œ± = 0.05)",
               answer("Nullhypothese beibehalten", 
                      message = "Da *p* mit 0.00‚Ä¶22 kleiner als 0.05 ist, sollten wir die Nullhypothese verwerfen."),
               answer("Nullhypothese verwerfen", 
                      correct = TRUE, 
                      message = "Da *p* unglaublich klein ist, auf jeden Fall kleiner als das Signifikanzniveau von 0.05, ist es richtig, die Nullhypothese zu verwerfen."),
               allow_retry = TRUE
)
```

### APA Bericht

Super! Wir k√∂nnen also davon ausgehen, dass:

::: blau-nb
*"Der Durchmesser von Kirschb√§umen erkl√§rt einen signifikanten Anteil der Varianz 
des Volumens dieser B√§ume $(F(1, 29) = 419.4, p < .001)$."*
::: 

### Lokaltest

Der Lokaltest testet jeden Regressionskoeffizienten einzeln (und ist
deswegen besonders f√ºr die multiple lineare Regression wichtig, wo es mehrere
Pr√§diktoren gibt).

Im Fall der einfachen linearen Regression entspricht der Signifikanztest 
des Lokaltest dem Test f√ºr das Gesamtmodell, weil das gesamte Modell nur aus einem
Pr√§diktor besteht. 

Das l√§sst sich auch sch√∂n zeigen anhand der Beziehung zwischen der
$t$-Statistik aus dem Lokaltest und der $F$-Statistik aus dem
Gesamtmodelltest:

Generell gilt: $F = t^2$.

Der *t*-Wert f√ºr unseren $b_1$ betr√§gt 20.48 und der *F*-Wert des Gesamtmodells $419.4$

mit $t^2 = 20.48^2 = 419.4 = F$

Dennoch gibt uns der Lokaltest dar√ºber hinaus relevante Informationen
√ºber den linearen Zusammenhang der unabh√§ngigen und abh√§ngigen Variable. 
Daher schauen wir uns den Lokaltest im Output jetzt gemeinsam an.

#### R-Output

Die Ergebnisse des Lokaltests befinden sich im Abschnitt `Coefficients`:

```{r localtest_output, exercise = T, exercise.eval = TRUE, exercise.setup = "silentsetup"}
fit_centered <- lm(volume ~ diameter, data = trees)
summary(fit_centered)
```

Den $p$-Wert f√ºr den Test von $b_1$ finden wir in der Zeile `diameter`,
also der Variablenname des Pr√§diktors.Er befindet sich in
der Spalte `Pr(>|t|)`.

Er ist identisch mit dem Wert aus dem Gesamtmodelltest! Auch das st√ºtzt,
dass die Tests im Fall der einfachen linearen Regression √§quivalent
sind.

```         
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.046122   0.095290  -10.98 7.62e-12 ***
diameter     0.056476   0.002758   20.48  < 2e-16 *** <----- p-Wert
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Den Lokaltest f√ºr *Intercept* ignorieren wir, da es in der Regel nicht
interessant ist zu wissen, ob der Achsenabschnitt in Wirklichkeit 0
ist - das w√§re ja eigentlich gar kein Problem, deswegen m√ºssen wir das
auch nicht herausfinden.

Was f√ºr uns also z√§hlt ist: Da der Durchmesser das Volumen *signifikant* vorhersagt,
d√ºrfen wir auch diesen Wert interpretieren. Das hast du bereits in einem fr√ºheren 
Kapitel gelernt, aber nutze doch das Quizz, um dein Wissen zu festigen.

```{r questionuncentered}
question_radio("Wie w√ºrdest du $b_1$ = 0.056 jetzt interpretieren?",
                  answer("F√ºr jeden cm Baumdurchmesser steigt das Volumen um 0.056 m¬≥.", 
                         correct = TRUE, 
                         message = "$b_1$ ist der vorhergesagte Wert, wenn der Pr√§diktor den Wert 0 annimt. Da nun 0 dem Mittelwert entspricht, k√∂nnen wir von einem mittleren Durchmesser sprechen"),
                  answer("Wenn man das Volumen um eine Einheit erh√∂ht, steigt der Durchmesser um 0.056 cm", 
                         correct = FALSE, 
                         message = "Hier ist alles verdreht. Das Volumen ist unser Kriterium, also k√∂nnen wir daraus nicht den Durchmesser vorhersagen."),
                  answer("Bei einem Durchmesser von 0.056 cm nimmt das vorhergesagte Volumen um eine Einheit zu.", 
                         correct = FALSE, 
                         message = "Das stimmt leider nicht, denn die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also in diesem Fall m¬≥."),
                  allow_retry = TRUE,
               correct = "das generelle Schema lautet ‚ÄûWenn man Pr√§diktor (Durchmesser) um 1 Einheit (cm) erh√∂ht, um wie viel √§ndert sich dann die Vorhersage (Volumen um 0.056m¬≥)?"
                  )
```

So k√∂nnen wir das Ergebnis jetzt auch nach APA Standard berichten

### APA berichten

Wir f√ºgen der Aussage zum Gesamtmodell nun die spezifische Aussage √ºber unseren 
Pr√§diktor nach diesem Schema hinzu:

*b*="Estimate", *t*("df")="t-Wert", *p*<."Pr(>|t|)" 
(die Werte, wie hier *b*, kannst du auch im Flie√ütext angeben)

::: blau-nb
*"Der Durchmesser von Kirschb√§umen erkl√§rt einen signifikanten Anteil der Varianz 
des Volumens dieser B√§ume $R^2 =.96, F(1, 29) = 419.4, p < .001)$." F√ºr jeden cm 
Baumdurchmesser steigt das Volumen um .056 m¬≥ $t(29)=20.48, p<.001$*.
::: 

::: gelb
**Hinweis**: Bei einem **Intercept mit nat√ºrlichem Nullpunkt** w√ºrden wir bei 
einer einfachen Regression auch dies hier in die Interpretation und den Bericht 
einflie√üen lassen und analog zum $b_1$ mit der entsprechenden Statistik versehen.

‚ÄûF√ºr B√§ume deren Durchmesser 0 cm betr√§gt, ist das
vorhergesagte Holzvolumen -1.05 m¬≥ $t(29)=-10.98, p<.001$."
:::

DU hast jetzt erfolgreich verstanden:

-   <input type="checkbox" unchecked> warum wir unser Modell testen  </input>
-   <input type="checkbox" unchecked> was wir testen  </input>
-   <input type="checkbox" unchecked> wo du den $p$-Wert findest, sowohl 
    f√ºr Lokaltest als auch Gesamtmodelltest  </input>
-   <input type="checkbox" unchecked> wie du die Ergebnisse berichtest  </input>
    
::: aufgabe
√úbungsaufgaben dazu findest du in einem eigenen √úbungs-Kapitel, wo du
alles Gelernte an einer Reihe von *R-Outputs* testen kannst.
:::

Prima! Weiter geht es damit, wie du deine Daten und dein Modell visualisieren
kannst.

![](images/tree.gif)

## Visualisierungen

Es gibt viele Wege, eine Punktwolke mit Regressionsgerade in R
herzustellen.

Ich stelle hier nur zwei vor:

1.  Der schnelle Weg, mit *base R*
2.  Der sch√∂ne Weg, mit `ggplot`

Aber es ist unfassbar wichtig, mindestens einen dieser Wege zu k√∂nnen,
um schnell und unkompliziert visuell zu √ºberpr√ºfen, ob die Daten
√ºberhaupt einen linearen Zusammenhang haben, und auch, um zu sehen, wie
gut die Gerade die Datenpunkte "erkl√§rt".

### Der schnelle Weg mit base R

```{r, echo = TRUE}
fit <- lm(volume ~ diameter, data = trees) # Modell erstellen

plot(volume ~ diameter, data = trees) # Punktdiagramm erstellen

abline(fit) # Regressionsgerade ins Punktdiagramm einbinden
```

#### Code Breakdown

-   Modell erstellen mit `lm()` und abspeichern wie gewohnt.

-   `plot()`:

    Funktioniert genau wie `lm` mit der Formelschreibweise `y ~ x`

    (Pluspunkt f√ºr Konsistenz)

-   `abline()`:

    hei√üt literally $a$-$b$-Line und f√ºgt eine Gerade zum Plot hinzu.

    $a$ ist in dem Fall Intercept und $b$ Slope.

    Das aufregende und sch√∂ne ist: Wir k√∂nnen einfach das gesamte
    gespeicherte Modell `fit` als Argument √ºbergeben, und `abline()`
    wei√ü selbst, wo die Koeffizienten zu finden sind und liest sie aus.

#### √úbung

Damit es etwas spannender wird, verwenden wir mal einen anderen
Datensatz: In `airquality` findest du Daten der Luftqualit√§t in New York
im Sommer 1973. Unter anderem wurde die Windgeschwindigkeit in
Meilen pro Stunde, und die maximale Tagestemperatur in Grad Fahrenheit gemessen.

```{r slopeestim}
question_radio("Gib einen Tipp ab: Welche Steigung wird die Regressionsgerade vermutlich haben?",
               answer("positiv", message = "Leider nicht so wahrscheinlich - dann w√ºrde es hei√üer werden, wenn der Wind schneller weht."),
               answer("negativ", correct = T, message = "Sinnvoll, da h√∂here Windgeschwindigkeit eher zu niedrigeren Temperaturen f√ºhrt -> negativer Zusammenhang"))
```

::: aufgabe
Erstelle ein simples Punktdiagramm mitsamt Regressionsgerade
mit *base R*.

1.  Stelle ein Regressionsmodell namens `temp_model` auf, was
    Ver√§nderungen der Temperatur (`Temp`) durch Ver√§nderungen der
    Windgeschwindigkeit (`Wind`) erkl√§ren kann.
2.  Erstelle ein Punktdiagramm
3.  F√ºge die Regressionsgerade hinzu

Variablennamen: `Wind` = Windgeschwindigkeit `Temp` = Temperatur
Der Datensatz `airquality` folgt also nicht den *tidy-Data*-Prinzipien der Kleinschreibung 
von Variablennamen!
:::


```{r baser, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "base R Grafik"}

```

```{r baser-hint}
# passe den Code an: 
model <- lm(outcome ~ predictor, data = airquality) # Modell erstellen

plot(outcome ~ predictor, data = airquality) # Punktdiagramm erstellen

abline(model) # Regressionsgerade zum Plot hinzuf√ºgen
```

```{r baser-solution}
temp_model <- lm(Temp ~ Wind, data = airquality) # Modell erstellen

plot(Temp ~ Wind, data = airquality) # Punktdiagramm erstellen

abline(temp_model) # Regressionsgerade zum Plot hinzuf√ºgen
```

### Der sch√∂ne Weg mit ggplot

Und hier der zweite Weg mit `ggplot`:

```{r, echo = T}
library(ggplot2) # Paket laden

ggplot(trees, aes(x = diameter, y = volume)) +   # Daten und Mappings definieren
  geom_point() +                                 # Punktdiagramm
  geom_smooth(method = "lm", se = F) +           # Regressionsgerade
  theme_minimal() +                              # Schnick-Schnack
  labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)", title = "sp√§tbl√ºhende Traubenkirsche")
```

#### Code Breakdown

Ich setze hier Grundkenntnisse in ggplot voraus, da das den Rahmen
sprengen w√ºrde (siehe Tutorial "Visualisierung").

-   `geom_smooth()` ist das Geom, was wir verwenden um eine
    Regressionsgerade zu zeichnen. Der entscheidende Unterschied zu *base
    R* ist, dass wir vorher kein Modell aufstellen m√ºssen, sondern das
    wird automatisch von `geom_smooth()` intern √ºbernommen.

    -   `method = "lm"`: Da es auch viele andere m√∂gliche Modelle gibt,
        m√ºssen wir im Argument `method` angeben, dass wir die Funktion
        `lm` nutzen wollen, also ein **l**ineares **M**odell.

        Intern wird dadurch `lm()` aufgerufen, standardm√§√üig mit der
        Formel `y ~ x`. Das Ergebnis h√§ngt davon ab, welche Variablen
        wir auf $x$ mappen und auf $y$. Dieser Sachverhalt wird uns auch
        in einer neutralen Nachricht bewusst gemacht:
        `geom_smooth() using formula = 'y ~ x'`.

    -   `se = F`: **s**tandard **e**rror = **F**ALSE. Standardm√§√üig ist
        dieses Argument `TRUE`, und es wird automatisch ein
        Konfidenzintervall mit eingezeichnet, was wir aber im Moment nicht
        ben√∂tigen.

#### √úbung

::: aufgabe
Erstelle nun analog zur letzten √úbung eine Grafik mittels `ggplot`,
die den Einfluss von `Wind` auf die `Temp`eratur in einem Punktdiagramm mit
Regressionsgerade darstellt.

Die Daten befinden sich wieder im `airquality`-Datensatz.
:::

```{r airggplot, exercise = TRUE, exercise.cap = "ggplot Grafik"}

```

```{r airggplot-hint}
# passe den Code an
ggplot(data, aes(x = , y = )) +           # Datensatz und Mappings definieren
  geom_point() +                          # Punktdiagramm zeichnen
  geom_smooth(method = "", se = F) +      # Regressionsgerade zeichnen
  theme_minimal()                         # wei√üer Hintergrund

# Zusatzaufgabe:
                     
#  geom_smooth(method = "lm", se = F, color = "red")  
```

```{r airggplot-solution}
ggplot(airquality, aes(x = Wind, y = Temp)) +  # Datensatz und Mappings definieren
  geom_point() +                          # Punktdiagramm zeichnen
  geom_smooth(method = "lm", se = F) +    # Regressionsgerade zeichnen
  theme_minimal()                         # wei√üer Hintergrund

# Zusatzaufgabe:
                     
#  geom_smooth(method = "lm", se = F, color = "red")  
```

::: aufgabe
**\* Zusatzaufgabe**

√Ñndere die Farbe der Regressionsgeraden mit (`color = "red"`).
:::

Super, jetzt hast du gelernt, wie eine einfache lineare Regression
visuell dargestellt werden kann!


## √úbungskapitel

### Vorannahmen pr√ºfen 1

Hier findest du eine Reihe von Aufgaben zu Regressionen in R. Dabei wollen wir einen
*nat√ºrlichen* Verlauf zeigen: d.h. erst die Vorannahmen testen, dann ein Modell berechnen, 
letzte Vorannahmen bezogen auf die Modellresiduen pr√ºfen und dann die Ergebnisse 
interpretieren.

Hier nochmal die Vorannahmen, die wir jetzt pr√ºfen:

+-------------------------------------+--------------------------------+
| Vorannahme                          | √úberpr√ºfung                    |
+=====================================+================================+
| 1.  metrische Variablen             | Vorwissen √ºber die Erhebung    |
+-------------------------------------+--------------------------------+
| 2.  Zuf√§llige Stichprobe            | Vorwissen √ºber die Erhebung    |
+-------------------------------------+--------------------------------+
| 3.  Linearer Zusammenhang in den    | Streudiagramm                  |
|     Daten                           |                                |
+-------------------------------------+--------------------------------+
| 4.  Es gibt Variation beim          | Streudiagramm                  |
|     Pr√§diktor                       |                                |
+-------------------------------------+--------------------------------+

::: aufgabe
Erstelle ein Streudiagramm f√ºr `trees$volume` und `trees$diameter`, das f√ºr die √úberpr√ºfung der Annahmen 3 und 4 auch schnell mit dem Befehl `plot(variable1,variable2)` erstellt werden kann. 
::: 

```{r vorannahmenuebung, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Vorannahmen pr√ºfen"}

```

```{r vorannahmenuebung-solution}
plot(trees$volume, trees$diameter)
```

```{r vorannahmen_questions}
question_checkbox("Welche der Vorannahmen w√ºrdest du nun als gegeben ansehen?",
                  answer("metrische Variablen", 
                         correct = T, 
                         message = "Beide Variablen Volumen und Durchmesser sind metrisch."),
                  answer("Zuf√§llige Stichprobe", 
                        correct = T, 
                        message = "Da die B√§ume nach keinem uns bekannten Muster 
                        ausgew√§hlt wurden, k√∂nnen wir von
                        einer zuf√§lligen Stichprobe ausgehen."),
                  answer("Linearer Zusammenhang in den Daten",
                        correct = T, 
                        message = "Der Scatterplot zeigt einen linearen Zusammenhang."),
                  answer("Es gibt Variation beim Pr√§diktor", 
                         correct = T, 
                         message = "Die Variation im Pr√§diktor ist ungleich 0."),
                  allow_retry = T
)
```

Super! 

### Modell berechnen 

Da du nun aber schon unsere Daten interpretiert hast. Nehmen wir f√ºr den jetztigen
Abschnitt Berechnung noch ein anderes Beispiel her. Wir hatten f√ºr die Korrelation
bereits den Datensatz `mtcars` genutzt, und den Zusammenhang von der Reichweite pro 
Gallone und dem Gewicht untersucht. Jetzt k√∂nnen wir testen, ob das Gewicht die
Varianz in der Reichweite erkl√§ren kann:

::: aufgabe
Berechne ein Modell (`car_fit`) zur Erkl√§rung der Reichweite (`mpg`) anhand des Gewichts (`wt`).
Lasse dir anschlie√üend die Statistiken ausgeben (`summary()`):

-   `mpg` - Miles per Gallon, Reichweite von Autos pro Gallone
    Kraftstoff (h√∂herer Wert = sparsamer!)
-   `wt` - weight - Gewicht in 1000lbs
:::

</br>

```{r ueb1_routput, exercise = TRUE, exercise.cap = "Regression berechnen"}

```

```{r ueb1_routput-hint}
# passe den code an:
car_fit <- lm(outcome ~ predictor, data = mtcars) 
# erstelle eine summary

```

```{r ueb1_routput-solution}
car_fit <- lm(mpg ~ wt, data = mtcars)  
summary(car_fit)
```

Bevor wir zur Interpretation kommen, sollten wir anhand des Outputs √ºberpr√ºfen,
ob unser Modell siginikant zur Erkl√§rung der Varianz der abh√§ngigen Variable beitr√§gt.

```{r signifikanz_pruefen}
question_checkbox("Was sollten wir auf Signifikanz √ºberpr√ºfen?",
                  answer("das Gesamtmodell", 
                         correct = T, 
                         message = "Richtig, wir testen das Gesamtmodell, um zu sehen, 
                         ob wir nicht ein Modell haben, dass nur zuf√§llig die Varianz in der
                         abh√§ngigen Variable aufkl√§rt."),
                  answer("das Lokalmodell",
                        correct = T, 
                        message = "Da unser Modell nur einen Pr√§diktor hat, ist mit der 
                        Sigifikanz des Gesamtmodells auch das Lokalmodell signifikant."),
                  allow_retry = T
)
```

### Modell interpretieren

```{r ueb_1a}
question_text("√úberlege dir die inhaltliche Interpretation der beiden Regressionskoeffizienten! (Intercept = 37.29, weight = -5.34)",
              answer_fn(~ correct()),
              placeholder = "Eine Musterl√∂sung erscheint, wenn du die Antwort einreichst, bitte vergleiche selbstst√§ndig.",
              correct = "Musterl√∂sung: \n
b0: F√ºr ein Gewicht von 0 lbs sagt das Modell 37.29 Meilen pro Gallone Treibstoff vorher. Das ist inhaltlich nat√ºrlich sinnlos, aber technisch ist das die richtige Interpretation. \n
b1: Wenn man das Gewicht um 1000 lbs erh√∂ht, verringert sich die vorhergesagte Reichweite pro Gallone um -5.34 Meilen.")
```

```{r ueb_1b}
question_text("Interpretiere die Modellg√ºte!",
  answer_fn(~ correct()),
              correct = "Musterl√∂sung: \n
R¬≤ betr√§gt 0.7528. Das bedeutet, das Modell erkl√§rt 75,28% der totalen Varianz der Reichweite. Das ist eine vergleichsweise hohe Varianzaufkl√§rung.")
```


```{r ueb_1c}
question_text("K√∂nnten wir die Ergebnisse auf eine gr√∂√üere Population √ºbertragen? Interpretiere die Ergebnisse des Lokal- und des Gesamtmodelltests. Welche Hypothesen werden jeweils getestet, und wie f√§llt Ihre Hypothesenentscheidung bei einem Signifikanzniveau von $\\alpha$ = 5% aus?",
  answer_fn(~ correct()),
              correct = "Musterl√∂sung: \n
Der Lokaltest testet die Nullhypothese, dass das Regressionsgewicht ‚ÄûGewicht‚Äú in Wahrheit 0 betr√§gt. Diese Hypothese verwerfe ich, da der *p*-Wert kleiner als Œ± ist. Das bedeutet, wir k√∂nnen davon ausgehen, dass die Stichprobe aus einer Population stammt, in der ‚ÄûGewicht‚Äú einen Einfluss auf die Reichweite hat. \n
Analog dazu testet der Gesamtmodelltest die Nullhypothese, dass das Modell in der Population in Wahrheit keine Varianzaufkl√§rung bietet. Da der *p*-Wert der gleiche ist, treffe ich hier die gleiche Hypothesenentscheidung und verwerfe die Nullhypothese, und gehe davon aus, dass das Modell auch in Wahrheit eine Varianzaufkl√§rung verschieden von 0 hat.")
```

Wow! Langsam wirst du doch schon richtig gut!

### Vorannahmen 2

Zur√ºck zu unseren Kirschb√§umen und den verbleibenden Vorannahmen. 
Wir haben unser Model als `fit` abgespeichert und wollen nun auch noch die letzten Tests durchf√ºhren.

+----------------------------------+-----------------------------------+
| Vorannahmen                      | √úberpr√ºfung                       |
+==================================+===================================+
| 5.  Homoskedastizit√§t            | `plot(resid(fit))`                |
+-------------------------------------+--------------------------------+
| 6.  Unkorreliertheit der         | `plot(fit, which = 1)`            |
|     Residuen mit der UV          |                                   |
+----------------------------------+-----------------------------------+
| 7.  keine Autokorrelation der    | `car::durbinWatsonTest()`         |
|     Residuen                     |                                   |
+----------------------------------+-----------------------------------+
| 8.  Normalverteilung der         | Q-Q-Plot: der zweite Plot bei     |
|     Residuen                     | `plot(fit)`                       |
|                                  |                                   |
|                                  | Histogramm der                    |
|                                  | standardisierten Residuen         |
|                                  |                                   |
|                                  | -`library(tidyverse)`             |
|                                  |                                   |
|                                  | -`resid(fit) |> scale() |> hist()`|                 
|                                  |                                   |
+----------------------------------+-----------------------------------+

Plotten wir doch zuest unser erstelltes Modell, um die **Homoskedastizit√§t** und **die 
Unkorreliertheit der Residuen mit der unabh√§ngigen Variable** zu √ºberpr√ºfen. Daf√ºr kannst du in der Base R Funktion einfach den Modellnamen `fit` eingeben und das Modell wird visualisiert. (Mit dem zus√§tzlichen Argument
`which` lassen wir uns nur die ersten beiden Plots ausgeben).

```{r ueb2_voraussetzungen, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Voraussetzungen pr√ºfen"}
fit <- lm(volume ~ diameter, data = trees)
plot(fit, which = 1,2)
lmtest::bptest(fit)
```

Plot 1:
Hier siehst du die Residuen abgetragen gegen unsere gesch√§tzen Werte f√ºr die abh√§ngige Variable. 
Das Wichtigste ist, dass die roten Linie, die den Mittelwert der Residuen darstellen, im Grunde horizontal und um Null zentriert ist. Denn das sagt uns, dass die Residuen (also die Varianz die vom Modell nicht erkl√§rt werden kann) keine Heteroskedasizit√§t vorliegt.
Willst du auf nummer sicher gehen kannst du auch analytisch ermitteln, ob Homoskedastizit√§t vorliegt. Daf√ºr kannst du das Modell in die Funktion `bptest()` aus dem Paket `lmtest` geben:

```{r bptest, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Homoskedasitzit√§t pr√ºfen"}
fit <- lm(volume ~ diameter, data = trees)
# Breusch-Pagan-Test H0: Homoskedastizit√§t
lmtest::bptest(fit)
```


```{r plotwich2, message = F, echo = F, warning = F}
fit <- lm(volume ~ diameter, data = trees)
plot(fit, which = 2)
```


```{r vorannahmenzwei_questions}
question_checkbox("Welche der Vorannahmen w√ºrdest du anhand des plots als gegeben ansehen?",
                  answer("Homoskedastizit√§t", 
                         correct = T, 
                         message = ""),
                  answer("Unkorreliertheit der Residuen mit der unabh√§ngigen Variable", 
                        correct = T, 
                        message = ""),
                  allow_retry = T
)
```

Als n√§chstes testen wir die Autokorrelation der Residuen mit dem `car::durbinWatsonTest(model)`.
Du merkst vielleicht bereits, dass es super angenehm ist, dass die Funktionen das Modell als Input nehmen. Mal wieder ein Grund warum *R* einfach toll ist ü•∞.

```{r ueb3_voraussetzungen, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Voraussetzungen pr√ºfen"}
fit <- lm(volume ~ diameter, data = trees)
car::durbinWatsonTest(fit)
```

```{r vorannahmendrei_questions}
question_radio("Ist die Vorannahmen erf√ºllt, dass die Residuen keine Autokorrelation vorweisen?",
                  answer("Ja.", 
                         correct = T, 
                         message = ""),
                  answer("Nein.", 
                        correct = T, 
                        message = ""),
                  allow_retry = T
)
```


```{r ueb4_voraussetzungen, exercise = TRUE, exercise.setup = "silentsetup", exercise.cap = "Voraussetzungen pr√ºfen"}
fit <- lm(volume ~ diameter, data = trees)
resid(fit) |> 
  scale() |> 
  hist()
```

```{r vorannahmenvier_questions}
question_checkbox("Ist die Vorannahmen erf√ºllt, dass die Residuen normalverteilt sind (H0)?",
                  answer("Ja!", 
                         correct = T, 
                         message = "..."),
                  answer("Nein!", 
                        message = "..."),
                  allow_retry = T
)
```

## EX: Ausblick in die Multiple lineare Regression

In der Praxis wird selten nur eine einfache (im Sinne von 1-fach)
lineare Regression gerechnet, sondern mehrere Pr√§diktoren werden
verwendet, um ein Kriterium vorherzusagen oder zu erkl√§ren.

Bei unserem Forst-Beispiel ist das ja auch so: Die **H√∂he der B√§ume** spielt nat√ºrlich
auch eine wichtige Rolle, und ist nicht zu vernachl√§ssigen, wenn wir
einen guten Sch√§tzer f√ºr das Holzvolumen eines Baumes haben wollen.

```{r multi}
trees$Height <- trees$Height * 0.3048  # Umrechnung Fu√ü in Meter

multiple_fit <- lm(volume ~ diameter + Height, data = trees)
summary(multiple_fit)
anova(fit, multiple_fit)
```

## Ex: Zentrieren

Zuerst ein paar Worte √ºber Transformationen:

Ab dem Intervallskalenniveau sind Transformationen zul√§ssig 
(aber nicht zwingend notwendig), solange sie die Proportion erhalten. 
F√ºr die lineare Regression m√ºssen das Kriterium $y$ und der Pr√§diktor 
$x$ beide metrisch sein, sind also mindestens intervallskaliert.

| Transformation | Daten        |
|----------------|--------------|
| $x$ (Original) | 10, 20, 30   |
| $x \cdot 5$    | 50, 100, 150 |
| $x - 20$       | -10, 0, 10   |

: Beispiele f√ºr proportionale Transformation:

Eine dieser proportionserhaltenden Transformationen ist das **Zentrieren**.

### Zentrieren

Beim Zentrieren wird **von jedem Wert der Mittelwert abgezogen**. Das
entspricht der letzten Tabellenzeile, wenn 20 der Mittelwert der
Zahlenreihe ist. Die **transformierten Daten bilden** dann **die Differenz zum
Mittelwert ab**.

#### Formel

$$
x' = x - \bar x
$$ 

Du bist dran!

::: aufgabe
**1.** Zentriere die Variable `diameter` aus dem `trees`-Datensatz
und speichere das Ergebnis als eine neue Spalte namens
`diameter_centered` ab.
:::

```{r center, exercise = TRUE, exercise.setup = "silentsetup", exercise.caption = "Zentrieren"}

```

```{r center-hint}
# nutze mean(trees$diameter) um den Mittelwert der Variable zu berechnen
```

```{r center-solution}
trees$diameter_centered <- trees$diameter - mean(trees$diameter)
```

```{r meancenter}
question_numeric("Was ist der Mittelwert einer *zentrierten* Variable? Pr√ºfen Sie notfalls im obigen Codeblock nach.",
                 answer(0, 
                        correct = TRUE, 
                        message = "Super! Der Mittelwert einer zentrierten Variablen ist immer 0, da wir die Daten so zentriert haben, dass sie die Abweichung zum Mittelwert beschreiben. Dabei weicht der Mittelwert logischerweise nicht von sich selbst ab: 20-20=0."),
                 incorrect = "Der Mittelwert einer zentrierten Variablen ist immer 0, da wir die Daten so zentriert haben, dass sie die Abweichung zum Mittelwert beschreiben. Dabei weicht der Mittelwert logischerweise nicht von sich selbst ab: 20-20=0.",
                 allow_retry = TRUE)
```

<details>

<summary><a>‚ñº mathematischer Beweis, was der Mittelwert einer
zentrierten Variablen sein muss</a></summary>

::: infobox
Es ist auch mathematisch zu zeigen, dass der Mittelwert einer
zentrierten Variable immer 0 ist:

```{=tex}
\begin{align}
\mbox{Allgemeine Formel f√ºr arithmetisches Mittel} & ~ & \bar x' = \frac{\sum_{i = 1}^n x_i'}{n} &= 0 \\
\mbox{Einsetzen der Zentrierungsformel} & ~ & \frac{\sum_{i = 1}^n (x_i - \bar x)}{n} &= 0 \\
\mbox{Aufspalten der Summe in mehrere Summen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n \bar x}{n} &= 0 \\
\mbox{\(\bar x\) ist eine Konstante - Summe einer Konstanten = \(n \cdot Konstante\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \bar x}{n} &= 0 \\
\mbox{Einsetzen der Formel f√ºr \(\bar x\)} & ~ & \frac{\sum_{i = 1}^n x_i - n \cdot \frac{\sum_{i = 1}^n x_i}{n}}{n} &= 0 \\
\mbox{K√ºrzen} & ~ & \frac{\sum_{i = 1}^n x_i - \sum_{i = 1}^n x_i}{n} &= 0 \\
\mbox{was zu zeigen war: 0 ist 0} & ~ & \frac{0}{n} &= 0
\end{align}
```
:::

</details>

#### Modell mit zentriertem Pr√§diktor

::: aufgabe
Rechne das Modell (`lm()`) noch einmal mit der *zentrierten* Variable `diameter_centered`. 
Gib dem Modell den Namen (`fit_centered`) und lasse dir daf√ºr die `summary()` ausgeben.
:::

```{r centernew, exercise = TRUE, exercise.setup = "silentsetup"}
trees$diameter_centered <- trees$diameter - mean(trees$diameter)

# berechnen des Modells


```

```{r centernew-hint}
trees$diameter_centered <- trees$diameter - mean(trees$diameter)

# die Funktion funktioniert wie folgt:
model <- lm(outcome ~ predictor, data = dataset)
summary(model)
```

```{r centernew-solution}
trees$diameter_centered <- trees$diameter - mean(trees$diameter)

fit_centered <- lm(volume ~ diameter_centered, data = trees)
summary(fit_centered)
```

```{r centerquestion}
quiz(
question_numeric("Wie lautet $b_0$ (*Intercept*) (auf drei Nachkommastellen)?",
                 answer(0.854347, 
                        correct = T),
                 answer(0.854, 
                        correct = T),
                        allow_retry = TRUE),
question_numeric("Wie lautet $b_1$ (*Slope*) (auf drei Nachkommestellen)?",
                 answer(0.056476, 
                        correct = T),
                 answer(0.056, 
                        correct = T),
                        allow_retry = TRUE),
caption = "Koeffizienten finden:")
```

Am Modell hat sich nichts ge√§ndert au√üer dem *Intercept*.

Stellen wir das visuell dar:

```{r echo=FALSE, message=FALSE, warning=FALSE}
uncentered <- ggplot(trees, aes(x = diameter, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm)", y = "Volumen (m¬≥)")

centered <- ggplot(trees, aes(x = diameter_centered, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_minimal() +
  labs(x = "Durchmesser (cm), zentriert", y = "Volumen (m¬≥)")

gridExtra::grid.arrange(uncentered, centered)
```

Die Steigung $b_1$ √§ndert sich durch das Zentrieren nicht.
**Nur die x-Achse ist anders skaliert**, die **Proportionen bleiben aber
erhalten**.

Bei den unzentrierten Daten ist 0 nicht im Wertebereich enthalten und
l√§sst sich auch nicht sinnvoll interpretieren.

Durch das Zentrieren haben wir 0 zu einem sinnvollen Wert gemacht, der
auch tats√§chlich durch die Daten abgedeckt ist, n√§mlich der Mittelwert! Das ist
f√ºr die Interpretation dann aber entsprechend zu beachten!


#### Interpretation zentrierte Pr√§diktoren

```{r questioncenter}
question_radio("Wie w√ºrdest du $b_0$ = 0.854 jetzt interpretieren, wo der Pr√§diktor zentriert wurde?",
                  answer("Bei einem mittleren Baumdurchmesser sagt das Modell 0.854 m¬≥ Holzernte voraus.", 
                         correct = TRUE, 
                         message = "$b_0$ ist der vorhergesagte Wert, wenn der Pr√§diktor den Wert 0 annimt. Da nun 0 dem Mittelwert entspricht, k√∂nnen wir von einem mittleren Durchmesser sprechen"),
                  answer("Wenn man das Volumen um eine Einheit erh√∂ht, steigt der mittlere Durchmesser um 0.854 cm", 
                         correct = FALSE, 
                         message = "Hier ist alles verdreht. Zun√§chst, das Volumen ist unser Kriterium, also k√∂nnen wir daraus nicht den Durchmesser vorhersagen. Aber selbst dann w√§re das Schema ‚ÄûWenn man Pr√§diktor um 1 Einheit erh√∂ht, um wie viel √§ndert sich dann die Vorhersage?‚Äú f√ºr die Interpretation f√ºr das Regressionsgewicht $b_1$ geeignet und nicht f√ºr die Regressionskonstante $b_0$. Zuletzt: Die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also ist cm hier auch nicht die richtige Einheit."),
                  answer("Bei einem hypothetischen Durchmesser von 0.854 cm nimmt das vorhergesagte Volumen den Mittelwert an", 
                         correct = FALSE, 
                         message = "Das stimmt leider nicht, denn die Koeffizienten liegen immer in Einheiten des Kriteriums vor, also in diesem Fall m¬≥. Und dann ist auch noch der Mittelwert an der falschen Stelle, n√§mlich eigentlich ist 0.854 das vorhergesagte Volumen in m¬≥ bei einem Durchmesser von 0 - da wir aber zentriert haben, entspricht das einem mittleren Durchmesser."),
                  allow_retry = TRUE
                  )
```

```{r importantcenter}
question_text("Hast du das Wichtigste mitgenommen? Was repr√§sentiert der Wert 0 bei einer zentrierten Variablen? (Stichwort)", 
                 answer("den Mittelwert", 
                        correct = T),
                 answer("arithmetisches Mittel", 
                        correct = T),
                 answer("Mittelwert", 
                        correct = T),                 
                 answer("mittelwert", 
                        correct = T),
            correct = "Super! Du hast das Wichtigste verstanden.", 
            incorrect = "Probieren Sie es nochmal (vielleicht eine andere Schreibweise). Zentrierte Daten haben als Wert ihren Abstand zum Mittelwert. Der Mittelwert hat den Abstand 0 zum Mittelwert",
            allow_retry = TRUE
)
```

Du bist schon sehr weit gekommen! Das kann gefeiert werden:

![](images/tree_dance.gif)


## Ex: Signifikanz erkl√§rt

Dieses Kaptiel erkl√§rt dir nochmal Schritt f√ºr Schritt was genau es mit dem 
Signifikanztest und *p*-Wert auf sich hat. Dabei wird als Beispiel der 
Signifikanztest des Gesamtmodells des `trees`-Datensatzes verwendet.

### Grundlage:

Was der Signifikanztest tut, ist zu schauen: Wie gro√ü ist die
Wahrscheinlichkeit $p$, die vorliegende Stichprobe (oder eine noch
extremere) aus einer Population zu ziehen, in der die Nullhypothese
gilt?

Wie gro√ü ist die Wahrscheinlichkeit, in den Wald zu gehen, 31 B√§ume zu
f√§llen, bei denen der Durchmesser zuf√§llig 93% der Varianz im Volumen
aufkl√§ren kann, obwohl im ganzen Wald der Durchmesser eigentlich nichts
mit dem Volumen zu tun hat?

Wenn es sich als sehr unwahrscheinlich herausstellt, gewinnen wir
gewisserma√üen r√ºckw√§rts die Erkenntnis, dass die Nullhypothese aller
Wahrscheinlichkeit nach nicht in der Population gilt.

Aber ab wann ist etwas sehr unwahrscheinlich?

Das sind arbitr√§r festgelegte Grenzen, die sich je nach Fachgebiet
unterscheiden, aber ein allgemeiner Standard ist ein Signifikanzniveau
$\alpha = 0.05\ \ (5\%)$.


### Interpretation des $p$-Werts

<details>

<summary><a>‚ñº Wie wird `2.2e-16` gelesen?</a></summary>

</br>

::: vorteile
Keine Panik - das ist wissenschaftliche Notation (*scientific
notation*).

Das wird verwendet, um sehr viele Nullen kompakt darzustellen.

So ist das Schema:

`2.2e-16`$= 2.2 \cdot 10^{-16} = 0\overbrace{.0000000000000002}^{\substack{\text{Komma} \\ \text{16 Stellen nach vorne}}}2$

-   `e` steht f√ºr ‚Äûmal 10 hoch Exponent‚Äú

-   `-16` ist der Exponent

Ein Positiv-Beispiel w√§re:

`5.12e+12`$= 5.12 \cdot 10^{12} = 5\overbrace{120\ 000\ 000\ 000}^{\substack{\text{Komma} \\ \text{12 Stellen nach hinten}}}.0$
:::

</details>

</br>

Der $p$-Wert ist eine Antwort auf folgende Frage: ‚ÄûUnter der Annahme,
dass $R^2$ in der Population 0 ist ‚Äî Wie wahrscheinlich ist es, zuf√§llig
eine Stichprobe zu ziehen in der das vorliegende oder ein noch
extremeres $R^2$ zustande kommt?‚Äú

> Angenommen, $\alpha = 0.05$:
>
> Wenn $p \le 0.05$ -\> unwahrscheinlich -\> Nullhypothese verwerfen
>
> Wenn $p > 0.05$ -\> wahrscheinlich -\> Nullhypothese beibehalten

```{r pquestion}
learnr::question_radio("Wie w√ºrde Ihre Hypothesenentscheidung f√ºr das Beispiel aussehen? (Œ± = 0.05)",
               answer("Nullhypothese beibehalten", message = "Da p mit 0.00‚Ä¶22 kleiner als 0.05 ist, sollten wir die Nullhypothese verwerfen."),
               answer("Nullhypothese verwerfen", correct = TRUE, message = "Da p unglaublich klein ist, auf jeden Fall kleiner als das Signifikanzniveau von 0.05, ist es richtig, die Nullhypothese zu verwerfen."),
               allow_retry = TRUE
)
```

#### Wiederholung p-Wert Interpretation

Da wir f√ºr die Errechnung des $p$-Werts eine Bedingung annehmen, n√§mlich
dass die Nullhypothese gilt, ist der $p$-Wert eine *bedingte*
Wahrscheinlichkeit.

Aussagen wie: <s>‚ÄûMit einer Wahrscheinlichkeit von [$p$ %] gilt die
Nullhypothese‚Äú</s> sind deshalb **falsch**. Ob die Nullhypothese in
Wahrheit gilt oder nicht, werden wir nie herausfinden - es sei denn, wir
messen die ganze Population - aber wir k√∂nnen sch√§tzen, wie
wahrscheinlich es ist, zuf√§llig das vorliegende Ergebnis oder ein
extremeres zu ziehen, wenn wir annehmen, dass die Nullhypothese gilt -
und daraus wiederum R√ºckschl√ºsse ziehen dar√ºber, ob wir die
Nullhypothese verwerfen oder beibehalten.

### Grafische Darstellung

Da $p$ in diesem Fall unglaublich klein ist, ist es schwierig, alle
relevanten Teile in einem Plot unterzubringen. Deswegen gibt es 3:

-   √úbersicht (komplett herausgezoomt)
-   Zoom Nr. 1
-   Zoom Nr. 2

```{r uebersichtplot, message=FALSE, warning=FALSE}

### Hilfsfunktion definieren, um Ausschnitte aus der F-Funktion zu generieren

.pshade <- function(x, min, max, df1, df2) {
    y <- df(x = x, df1 = df1, df2 = df2)
    y[x < min  |  x > max] <- NA
    return(y)
}

### Parameter definieren

alpha = 0.05
df1 = 1
df2 = 29
detail = 800  # Detailgrad des Plots
f_krit <- qf(1 - alpha, df1, df2)
f_emp <- 419.4  # aus dem R-output √ºbernommen
xlim <- f_krit + 2

### √úbersichts-Plot
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
  stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim, df1 = df1, df2 = df2),
                n = detail) +  
  geom_function(fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
  annotate("segment", x = f_emp, xend = f_emp, y = 0, yend = 0.1) +
  annotate("label", y = 0.1, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
  annotate("segment", xend = f_krit, x = 50, yend = 0, y = 0.1, 
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  annotate("label", x = 50, y = 0.1, label = "kritische\nGrenze") +
  xlim(0, f_emp + 100) +
  labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "√úbersicht", subtitle = "F-Verteilung") +
  theme_minimal()

```

In der √úbersicht sehen Sie den empirischen (=beobachteten) F-Wert, der
sich aus der Varianzaufkl√§rung $R^2$ des Modells berechnen l√§sst. Er
entspricht `F-statistic` im R-Output.

Hier noch mal die relevante Zeile aus dem Output:

```         
F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16
```

<details>

<summary><a>‚ñº \* Hintergrundwissen: Wie wird der $F$-Wert
berechnet?</a></summary>

</br>

::: infobox
```{=tex}
\begin{align*}
F &= \frac{R^2}{1-R^2} \\
F &= \frac{\text{durch das Modell erkl√§rte Varianz}}{\text{nicht erkl√§rbare Varianz}}
\end{align*}
```
:::

</details>

</br>

Die blaue Fl√§che repr√§sentiert 95% der Fl√§che unter der Kurve. Nach den
95% beginnen die restlichen 5% der Fl√§che, in gelb, und dazwischen liegt
die kritische Grenze. Ist der empirische F-Wert gr√∂√üer als die kritische
Grenze, ist er innerhalb der gelben Fl√§che, also innerhalb den √§u√üersten
5%. Ab diesem Punkt wird die Nullhypothese als "unwahrscheinlich"
verworfen - der Test wird signifikant.

```{r zoom1, message = F, warning = F}
### Zoom 1
ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_krit, xlim)), y = 0.1, xend = mean(c(f_krit, xlim)), yend = 0.008,
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = f_krit, y = 0.1, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.12, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim)), y = 0.12, label = "5 % der\nFl√§che") +
    annotate("label", x = 1, y = 0.1, label = "95 % der\nFl√§che") +
    scale_x_continuous(limits = c(0, xlim)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 1",
         subtitle = "Streckung der x-Achse") +
    theme_minimal()


```

#### Wo ist der $p$-Wert?

Die Fl√§che unter Kurve ab dem empirischen $F$-Wert bis $+\infty$
entspricht dem $p$-Wert.

Dieser ist in diesem Fall so klein, dass man erst sehr weit hineinzoomen
muss (siehe Zoom Nr. 2). Dort kann man den $p$-Wert als Fl√§che erkennen.
(Remember, im Output stand ja bereits, $p < 2.2 \cdot 10^{-16}$, was 15
Nullen nach dem Komma entspricht - in der √úbersicht √ºberhaupt nicht
sichtbar.)

```{r fplots, message=FALSE, warning=FALSE}
### Zoom 2

xlim_small <- f_emp + 100
ylim_small <- 1e-19
ggplot() +
    stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
      stat_function(geom = "area",
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_small, df1 = df1, df2 = df2),
                n = detail) +
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = mean(c(f_emp, xlim_small)), yend = 0.2e-20, xend = mean(c(f_emp, xlim_small)), y = 1e-20, # p-Pfeil
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 50, yend = 0, xend = f_krit, y = 2.5e-20, # Pfeil krit. Gr.
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 2.5e-20, label = "kritische\nGrenze") +           # Label kritische Grenze
    annotate("segment", x = 50, yend = 7.5e-20, xend = 2, y = 7.5e-20, # Pfeil krit. Gr. # Pfeil 95%
         arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("label", x = 50, y = 7.5e-20, label = "95 % der\nFl√§che") + # Label 95%
    annotate("label", x = mean(c(f_emp, xlim_small)), y = 1.2e-20, label = "Fl√§che\n= p-Wert") + # p-Label
    annotate("label", x = 200, y = 5e-20, label = "5 % der\nFl√§che") +
    annotate("segment", x = f_emp, y = 0, yend = 5e-20, xend = f_emp) +
    annotate("label", y = 5e-20, x = f_emp, label = "empirischer\nF-Wert\n= 419") +
    xlim(0, xlim_small) +
    coord_cartesian(ylim = c(0, ylim_small), xlim = c(0, xlim_small), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Zoom Nr. 2",
         subtitle = "Streckung der y-Achse") +
    theme_minimal()

```

#### zum Vergleich: nicht signifikant

Hier noch mal der entgegengesetzte Fall:

Falls der empirische $F$-Wert innerhalb der blauen 95% liegen w√ºrde, die
als ‚Äûwahrscheinlich‚Äú gelten - wie w√ºrde unsere Hypothenentscheidung dann
lauten?

Wir w√ºrden die Nullhypothese beibehalten, da
$p > \alpha = 5.21 \%> 5\%$.

Die Wahrscheinlichkeit, die beobachtete Stichprobe aus einer Population
zu ziehen, in welcher die Nullhypothese gilt, ist h√∂her als die von uns
angesetzte Grenze (Signifikanzniveau), ab der etwas als
‚Äûunwahrscheinlich‚Äú gilt.

```{r notsignificantplot, message=FALSE, warning=FALSE}
# devtools::install_github("NicolasH2/ggbrace")
# WARNING! GITHUB DEPENDENCY, wird wahrscheinlich nicht automatisch gehandled - momentan habe ich das testweise verlegt auf setup chunk
# library(ggbrace)

### Parameter festlegen
f_emp <- 3.0
xlim_new <- f_krit + 1
p <- df(3, df1, df2) # 0.052
### PLOT 
### Zoom 1
try(
  ggplot() +
  stat_function(geom = "area",
                fill = "4",
                alpha = .4,
                fun = .pshade, 
                args = list(min = 0, max = f_krit, df1 = df1, df2 = df2),
                n = detail) +
    stat_function(geom = "area",
                fill = "7",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_krit, max = xlim, df1 = df1, df2 = df2),
                n = detail) +
 stat_function(geom = "area",              # p-Wert
                fill = "red",
                alpha = .4,
                fun = .pshade, 
                args = list(min = f_emp, max = xlim_new, df1 = df1, df2 = df2),
                n = detail) +  
    geom_function(
            fun = df, args = list(df1 = df1, df2 = df2), n = detail) +
    annotate("segment", x = f_krit, y = 0.3, xend = f_krit, yend = 0) +
    annotate("label", x = f_krit, y = 0.3, label = "kritische\nGrenze") +
    annotate("label", x = mean(c(f_krit, xlim_new)), y = 0.04, label = "Œ± = 5 %") +
    annotate("label", x = 2.1, y = 0.05, label = "1 - Œ± = 95 %") +
    annotate("segment", x = f_emp, y = 0, yend = .3, xend = f_emp) +
    annotate("label", y = 0.3, x = f_emp, label = "empirischer\nF-Wert") + 
    annotate("label", y = .13, x = 3.63, label = "p = 5.21 %") +
    geom_brace(aes(x = c(0, f_krit), y = c(0.0, 0.03))) +
    geom_brace(aes(x = c(f_krit, xlim_new), y = c(0.0, 0.023))) +
    geom_brace(aes(x = c(f_emp, xlim_new), y = c(0.05, 0.115)), mid = 0.29) +
    
    scale_x_continuous(limits = c(0, xlim_new)) +
    coord_cartesian(ylim = c(0, 0.5), xlim = c(0, xlim_new), expand = F) +
    labs(x = "F(1, 29)", y = "Wahrscheinlichkeitsdichte", title = "Beispiel",
         subtitle = "nicht signifikant (p > Œ±)") +
    theme_minimal()
)

# Fail, does not render correctly in output
# library(pBrackets)
# grid.brackets(518, 453, 858, 453, h = .1)
# grid.brackets(702, 478, 858, 478, h = .09)
# grid.brackets(50, 478, 702, 487, h = .1, curvature = 0.2)


```


## Abschlussquiz


```{r novariation_abschluss, fig.height=5, fig.width=5}
plot(x = rep(20, 100), y = rnorm(100), yaxt = "n", xaxt = "n", xlab = "x", ylab = "y")
```

```{r proquestion}
question_numeric("Welchen Wert w√ºrde $R^2$ im obigen Plot annehmen?",
                 answer(0, correct = T),
                 allow_retry = T)
```

<details>

<summary><a>‚ñº Erkl√§rung</a></summary>

::: infobox
```{=tex}
\begin{align}
R^2 &= \frac{\text{Durch das Modell erkl√§rte Varianz in y}}{\text{Gesamtvarianz in y}} \\
R^2 &= \frac{0}{\text{Gesamtvarianz in y}} = 0
\end{align}
```
:::

</details>


## Learnings

## Abstellgleis

::: aufgabe
Wie hoch ist $e_{20}$ (das Residuum von Messung 20) beim
Eingangsbeispiel im `trees` Datensatz?

Berechnen Sie mit R!

$$
e_{20} = y_{20} - \hat y_{20}
$$
:::

```{r residuumberechnen, exercise = TRUE, exercise.cap = "Residuum"}
e20 <- trees$volume[20] - predict(fit)[20]
e20
resid(fit)[20]

```
